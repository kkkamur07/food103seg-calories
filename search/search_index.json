{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Food103Seg Calories","text":"<p>Welcome to Food103Seg - a deep learning project for food image segmentation.</p>"},{"location":"#live-demo","title":"\ud83d\ude80 Live Demo","text":"<p>Try the application now! Our Streamlit app is live at: the link</p>"},{"location":"#overview","title":"Overview","text":"<p>This project uses computer vision to identify different food items in images by segmenting them. We have Built this with PyTorch and by using a simplified MiniUNet architecture, it can segment 104 different food categories with moderate accuracy. We trained the MiniUNet architecture on RTX 4090 for about 20 Minutes and the hyperparameter configuration were determined by the hyperparameter sweep of wandb.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udf55 Food Segmentation: Identifies and segments different food items in images</li> <li>\ud83e\udde0 MiniUNet Model: Lightweight U-Net architecture optimized for food segmentation</li> <li>\ud83c\udf10 Web Interface: User-friendly Streamlit application</li> <li>\u2699\ufe0f Easy Configuration: Hydra-based configuration management</li> <li>\ud83d\udcc8 Experiment Tracking: Integration with Weights &amp; Biases</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/kkkamur07/food103seg-calories\n\n# Navigate to the project directory\ncd food103seg-calories\n\n# Create virtual environment using UV\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install the dependencies\nuv pip install -r requirements.txt\n\n# Train the model\npython -m src.segmentation.main\n\n# Launch API Server\nuvicorn src.app.service:app --host 0.0.0.0 --port 8000 --reload\n\n# Launch Web App\nstreamlit run src/app/frontend.py\n</code></pre>"},{"location":"#mlops-template","title":"MLOps Template","text":"<p>We provide a ready-to-use MLOps template for this project structure:</p> <pre><code># Install cookiecutter\npip install cookiecutter\n\n# Generate project using our template\ncookiecutter https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Find the complete template and installation guide at:</p> <pre><code>https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Sources</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 segmentation/         # Core ML modules\n\u2502   \u2502   \u251c\u2500\u2500 data.py           # Data loading\n\u2502   \u2502   \u251c\u2500\u2500 model.py          # MiniUNet architecture\n\u2502   \u2502   \u251c\u2500\u2500 train.py          # Training pipeline\n\u2502   \u2502   \u2514\u2500\u2500 main.py           # Main training script\n\u2502   \u2514\u2500\u2500 app/                  # Web application\n\u2502       \u251c\u2500\u2500 frontend.py       # Streamlit interface\n\u2502       \u2514\u2500\u2500 service.py        # API service\n\u251c\u2500\u2500 configs/                  # Configuration files and management\n\u251c\u2500\u2500 notebooks/                # Jupyter notebooks for experiments\n\u2514\u2500\u2500 docs/                     # Documentation\n</code></pre> <p>We have also used <code>bentoML</code> for optimized API for ML models specially, this can be found in.</p> <pre><code>\u251c\u2500\u2500 src/\n    \u251c\u2500\u2500 segmentation/\n        \u251c\u2500\u2500 bentoml.py        # For BentoML\n        \u251c\u2500\u2500 bentoml_setup.py  # For setting up BentoML\n\n</code></pre> <p>Serve the API locally using BentoML</p> <pre><code>bentoml serve src/segmentation/bentoml:latest\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Installation - Set up the project</li> <li>API Reference - Explore the code</li> <li>Training Guide - Train your own models</li> </ol>"},{"location":"#model-performance","title":"Model Performance","text":"<ul> <li>104 Food Classes: Comprehensive food category coverage</li> <li>Moderate Accuracy: &gt;20% mean IoU on test set</li> <li>Moderate Inference Speed: ~100ms per image on GPU</li> <li>Lightweight: ~15MB model size</li> </ul> <p>Ready to start? Check out our installation guide or try the live demo!</p> <p>Sources</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the Food103Seg Calories project, ensure you have the following prerequisites:</p> <ul> <li>Python 3.8+ (Python 3.9 or 3.10 recommended)</li> <li>CUDA-compatible GPU (recommended for training, optional for inference)</li> <li>Git for cloning the repository</li> <li>pip package manager</li> </ul>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"Component Minimum Recommended Python 3.8+ 3.9 or 3.10 RAM 8GB 16GB+ GPU Memory 4GB 8GB+ Storage 10GB 20GB+"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#setup-instructions-using-uv","title":"Setup Instructions Using uv","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/kkkamur07/food103seg-calories\ncd food103seg-calories\n</code></pre>"},{"location":"installation/#2-create-virtual-environment-and-install-dependencies","title":"2. Create Virtual Environment and Install Dependencies","text":"<p>Using uv (recommended for fastest setup):</p> <pre><code># Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install production dependencies\nuv pip install -r requirements.txt\n\n# Install development dependencies (optional)\nuv pip install -r requirements_dev.txt\n\n# Install project in development mode\nuv pip install -e .\n</code></pre> <p>Alternative one-liner approach:</p> <pre><code># Create environment and install dependencies in one step\nuv venv &amp;&amp; source .venv/bin/activate &amp;&amp; uv pip install -r requirements.txt\n</code></pre>"},{"location":"installation/#3-gpu-setup-optional-but-recommended","title":"3. GPU Setup (Optional but Recommended)","text":"<p>For CUDA support, install PyTorch with CUDA using uv:</p> <pre><code># For CUDA 11.8\nuv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# For CUDA 12.1\nuv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>Verify GPU installation:</p> <pre><code>python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"installation/#about-the-requirements-files","title":"About the Requirements Files","text":"<p>This project provides two dependency files for different use cases[1][2]:</p> <ul> <li><code>requirements.txt</code> - Contains production dependencies needed to run the application</li> <li><code>requirements_dev.txt</code> - Contains additional development dependencies for testing, linting, and development tools[3]</li> </ul> <p>The uv package manager provides significant speed improvements over traditional pip installations[4][2], making it ideal for projects with multiple dependencies. You can install from either file using <code>uv pip install -r &lt;filename&gt;</code>[5].</p>"},{"location":"installation/#data-setup","title":"Data Setup","text":""},{"location":"installation/#1-download-dataset","title":"1. Download Dataset","text":""},{"location":"installation/#data-storage-and-versioning-with-dvc","title":"Data Storage and Versioning with DVC","text":"<p>This project uses DVC (Data Version Control) with Google Cloud Storage for data versioning and management. The data and models are stored in two separate GCS buckets:</p> <ul> <li>Data storage: <code>gs://dvc-storage-sensor/</code></li> <li>Model storage: <code>gs://food-segmentation-models/</code></li> </ul>"},{"location":"installation/#setting-up-dvc-with-google-cloud-storage","title":"Setting Up DVC with Google Cloud Storage","text":"<pre><code># Create data directory\nmkdir -p data\n\n# Install required tools\npip install dvc-gs\n\n# List available GCS buckets\ngsutil ls\n\n# Add remote storage (replace &lt;output-from-gsutils&gt; with actual bucket path)\ndvc remote add -d remote_storage gs://dvc-storage-sensor/\n\n# Configure version-aware storage\ndvc remote modify remote_storage version_aware true\n\n# List configured remotes\ndvc remote list\n\n# Pull data from remote storage\ndvc pull\n</code></pre>"},{"location":"installation/#dvc-management-commands","title":"DVC Management Commands","text":"<pre><code># Remove a remote if needed\ndvc remote remove gcp_storage\n\n# Set default remote\ndvc remote default remote_storage\n\n# Push data (note: --no-cache may have issues)\ndvc push --no-cache\n</code></pre>"},{"location":"installation/#known-issues-with-dvc-setup","title":"Known Issues with DVC Setup","text":"<p>During development, several challenges were encountered with the DVC workflow, particularly with the <code>dvc push --no-cache</code> command. While DVC provides excellent data versioning capabilities, the setup proved complex for this project's requirements.</p>"},{"location":"installation/#alternative-direct-dataset-download","title":"Alternative: Direct Dataset Download","text":"<p>If you prefer to bypass the DVC setup or encounter issues, you can download the Food103 segmentation dataset directly from:</p> <p>Dataset source: https://paperswithcode.com/dataset/foodseg103</p> <pre><code># Create data directory\nmkdir -p data\n\n# Download dataset manually from Papers with Code\n# Extract and place in data/ directory\n</code></pre>"},{"location":"installation/#recommended-approach","title":"Recommended Approach","text":"<p>For this project, you can choose either approach:</p> <ol> <li>DVC approach - Use the GCS buckets with DVC for version control</li> <li>Direct download - Download the Food103 dataset directly from Papers with Code</li> </ol> <p>The DVC setup provides better data versioning and collaboration features, while the direct download approach is simpler for getting started quickly.</p>"},{"location":"installation/#2-expected-data-directory-structure","title":"2. Expected Data Directory Structure","text":"<p>Ensure your data follows this structure:</p> <pre><code>data/\n\u251c\u2500\u2500 Images/\n\u2502   \u251c\u2500\u2500 img_dir/\n\u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 test/\n\u2502   \u2502       \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502       \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ann_dir/\n\u2502       \u251c\u2500\u2500 train/\n\u2502       \u2502   \u251c\u2500\u2500 image1.png\n\u2502       \u2502   \u251c\u2500\u2500 image2.png\n\u2502       \u2502   \u2514\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 test/\n\u2502           \u251c\u2500\u2500 image1.png\n\u2502           \u251c\u2500\u2500 image2.png\n\u2502           \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"installation/#configuration-setup","title":"Configuration Setup","text":""},{"location":"installation/#copy-the-template","title":"Copy the Template","text":"<pre><code># Install cookiecutter\npip install cookiecutter\n\n# Generate project using our template\ncookiecutter https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Find the complete template and installation guide at:</p> <pre><code>https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Sources</p>"},{"location":"installation/#verification","title":"Verification","text":""},{"location":"installation/#1-test-installation","title":"1. Test Installation","text":"<p>Run the following commands to verify your installation:</p> <pre><code># Test imports\npython -c \"import torch; import torchvision; print('PyTorch installed successfully')\"\n\n# Test project modules\npython -c \"from src.segmentation.data import data_loaders; print('Project modules working')\"\n\n# Test data loading\npython -c \"from src.segmentation.data import data_loaders; print('Data loading test passed')\"\n</code></pre>"},{"location":"installation/#2-quick-training-test","title":"2. Quick Training Test","text":"<p>Run a quick training test with minimal epochs:</p> <pre><code>python src/segmentation/main.py model.hyperparameters.epochs=1\n</code></pre>"},{"location":"installation/#running-the-application","title":"Running the Application","text":""},{"location":"installation/#1-streamlit-web-app","title":"1. Streamlit Web App","text":"<pre><code>streamlit run src/app/frontend.py\n</code></pre>"},{"location":"installation/#2-api-server-fastapi-with-uvicorn","title":"2. API Server (FastAPI with Uvicorn)","text":"<pre><code>uvicorn src.app.api:app --host 0.0.0.0 --port 8000 --reload\n</code></pre>"},{"location":"installation/#3-training-pipeline","title":"3. Training Pipeline","text":"<pre><code>python src/segmentation/main.py\n</code></pre>"},{"location":"installation/#4-custom-training","title":"4. Custom Training","text":"<pre><code>python src/segmentation/main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001\n</code></pre>"},{"location":"installation/#access-points","title":"Access Points","text":"<ul> <li>Web App: <code>http://localhost:8501</code></li> <li>API: <code>http://localhost:8000</code></li> <li>API Docs: <code>http://localhost:8000/docs</code></li> </ul>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":""},{"location":"installation/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<ul> <li>Reduce batch size in config: <code>model.hyperparameters.batch_size=16</code></li> </ul>"},{"location":"installation/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code>pip install --upgrade pip\npip install -r requirements.txt --force-reinstall\n</code></pre>"},{"location":"installation/#data-loading-errors","title":"Data Loading Errors","text":"<ul> <li>Verify directory structure matches expected format</li> <li>Check file permissions: <code>chmod -R 755 data/</code></li> <li>Ensure image and annotation files have correct extensions</li> </ul>"},{"location":"installation/#import-errors","title":"Import Errors","text":"<pre><code># Reinstall in development mode\npip install -e . --force-reinstall\n</code></pre>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the logs in <code>saved/logs/</code></li> <li>Verify GPU setup with <code>nvidia-smi</code></li> <li>Review configuration in <code>configs/config.yaml</code></li> <li>Check Python version compatibility</li> </ol>"},{"location":"installation/#optional-components","title":"Optional Components","text":""},{"location":"installation/#docker-setup","title":"Docker Setup","text":"<p>If you prefer Docker:</p> <pre><code># Build backend\ndocker build -f Dockerfile.backend -t food-seg-backend .\n\n# Build frontend\ndocker build -f Dockerfile.frontend -t food-seg-frontend .\n\n# Run with docker-compose\ndocker-compose up\n</code></pre>"},{"location":"installation/#development-tools","title":"Development Tools","text":"<p>Install additional development tools:</p> <pre><code># Pre-commit hooks\npre-commit install\n\n# Jupyter for notebooks\npip install jupyter\njupyter notebook notebooks/\n\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Review the configuration in <code>configs/config.yaml</code></li> <li>Run the training pipeline with your data</li> <li>Explore the Streamlit app at the live link above</li> <li>Check the documentation for advanced usage</li> <li>Set up monitoring with Weights &amp; Biases</li> </ol> <p>You're now ready to start training your food segmentation model and estimating calories! \ud83c\udf55\ud83d\udcca</p>"},{"location":"structure/","title":"Project Structure","text":"<pre><code>kkkamur07-food103seg-calories/\n\u251c\u2500\u2500 README.md                          # Project overview and instructions\n\u251c\u2500\u2500 cloudbuild.yaml                    # Google Cloud Build config\n\u251c\u2500\u2500 data.dvc                           # DVC-tracked data file\n\u251c\u2500\u2500 docker-compose.yml                 # Orchestration of backend + frontend\n\u251c\u2500\u2500 Dockerfile.backend                 # Dockerfile for backend service\n\u251c\u2500\u2500 Dockerfile.frontend                # Dockerfile for frontend app\n\u251c\u2500\u2500 pyproject.toml                     # Python project metadata + build system\n\u251c\u2500\u2500 requirements.txt                   # Production dependencies\n\u251c\u2500\u2500 requirements_dev.txt               # Dev dependencies (linting, testing)\n\u251c\u2500\u2500 tasks.py                           # Automation scripts (e.g. via `invoke`)\n\u251c\u2500\u2500 uv.lock                            # Dependency lock file for uv tool\n\u251c\u2500\u2500 wandb_runner.py                    # W&amp;B experiment runner for hyperparameter sweep\n\u251c\u2500\u2500 .dockerignore                      # Ignore rules for Docker builds\n\u251c\u2500\u2500 .dvcignore                         # Ignore rules for DVC\n\u251c\u2500\u2500 .pre-commit-config.yaml            # Pre-commit hooks config\n\u251c\u2500\u2500 configs/                           # All project configs\n\u2502   \u251c\u2500\u2500 config.yaml                    # Main config file (training, paths)\n\u2502   \u251c\u2500\u2500 wandb_sweep.yaml               # W&amp;B sweep configuration\n\u2502   \u251c\u2500\u2500 dataset/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml               # Dataset-specific config\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml               # Model-specific config\n\u2502   \u2514\u2500\u2500 outputs/                       # Experiment outputs\n\u2502       \u251c\u2500\u2500 2025-07-02/\n\u2502       \u2502   \u2514\u2500\u2500 22-43-00/\n\u2502       \u2502       \u251c\u2500\u2500 wandb/             # W&amp;B run logs\n\u2502       \u2502       \u2514\u2500\u2500 .hydra/            # Hydra config snapshots\n\u2502       \u2502           \u251c\u2500\u2500 config.yaml\n\u2502       \u2502           \u2514\u2500\u2500 hydra.yaml\n\u2502       \u2514\u2500\u2500 2025-07-04/\n\u2502           \u2514\u2500\u2500 21-10-21/\n\u2502               \u2514\u2500\u2500 .hydra/\n\u2502                   \u2514\u2500\u2500 hydra.yaml\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 experiment.ipynb              # Jupyter notebook for experiments\n\u251c\u2500\u2500 saved/\n\u2502   \u2514\u2500\u2500 models.dvc                    # Tracked model weights &amp; biases file(s) with DVC\n\u251c\u2500\u2500 src/                              # Source code\n\u2502   \u251c\u2500\u2500 app/                          # Application code (serving)\n\u2502   \u2502   \u251c\u2500\u2500 bentoml.py                # BentoML service definition\n\u2502   \u2502   \u251c\u2500\u2500 bentoml_setup.py          # BentoML setup utility\n\u2502   \u2502   \u251c\u2500\u2500 frontend.py               # Streamlit or Gradio frontend\n\u2502   \u2502   \u251c\u2500\u2500 frontend_requirements.txt\n\u2502   \u2502   \u2514\u2500\u2500 service.py                # Service logic\n\u2502   \u251c\u2500\u2500 segmentation/                 # Core ML logic\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 data.py                   # Dataset loading, transforms\n\u2502   \u2502   \u251c\u2500\u2500 loss.py                   # Custom loss functions\n\u2502   \u2502   \u251c\u2500\u2500 main.py                   # Entrypoint script\n\u2502   \u2502   \u251c\u2500\u2500 model.py                  # Model architectures\n\u2502   \u2502   \u2514\u2500\u2500 train.py                  # Training loop\n\u2502   \u2514\u2500\u2500 tests/                        # Tests\n\u2502       \u251c\u2500\u2500 test_data1.py\n\u2502       \u251c\u2500\u2500 test_model.py\n\u2502       \u251c\u2500\u2500 test_training.py\n\u2502       \u251c\u2500\u2500 tests_integration/        # Integration-level tests\n\u2502       \u2502   \u251c\u2500\u2500 api_testing.py\n\u2502       \u2502   \u2514\u2500\u2500 locustfile.py         # Load testing with Locust\n\u2502       \u2514\u2500\u2500 tests_unit/               # Unit-level tests\n\u2502           \u251c\u2500\u2500 test_data.py\n\u2502           \u2514\u2500\u2500 test_train.py\n\u2502\u2500\u2500 report/                           # Exam report folder\n\u2502   \u251c\u2500\u2500 README.md                     # Exam answers\n\u2502   \u251c\u2500\u2500 figures/                      # Images for report\n\u2502   \u2514\u2500\u2500 report.py                     # Report generation script\n\u251c\u2500\u2500 favicon.py                        # API favicon\n\u251c\u2500\u2500 static/                           # Static files\n\u2502   \u251c\u2500\u2500 favicon.ico\n\u2514\u2500\u2500 .github/                          # GitHub CI/CD config\n    \u251c\u2500\u2500 dependabot.yaml               # Dependency update config\n    \u2514\u2500\u2500 workflows/                    # GitHub Actions workflows\n        \u251c\u2500\u2500 ci.yml                    # Main CI pipeline\n        \u251c\u2500\u2500 data-changes.yaml         # DVC-based data CI triggers\n        \u2514\u2500\u2500 model-deploy.yml          # DVC-based model W&amp;B CI triggers\n</code></pre>"},{"location":"source/data/","title":"Data Module","text":"<p>This module handles the data loading from the datasets and data version control and preprocessing for food segmentation.</p>"},{"location":"source/data/#dvc-setup-and-configuration","title":"DVC Setup and Configuration","text":""},{"location":"source/data/#data-storage-architecture","title":"Data Storage Architecture","text":"<p>The project uses DVC (Data Version Control) with Google Cloud Storage for data versioning and management:</p> <ul> <li>Data storage: <code>gs://dvc-storage-sensor/</code></li> <li>Model storage: <code>gs://food-segmentation-models/</code></li> </ul>"},{"location":"source/data/#initial-dvc-setup","title":"Initial DVC Setup","text":"<pre><code># Install required tools\npip install dvc-gs\n\n# List available GCS buckets\ngsutil ls\n\n# Add remote storage\ndvc remote add -d remote_storage gs://dvc-storage-sensor/\n\n# Configure version-aware storage\ndvc remote modify remote_storage version_aware true\n\n# Pull data from remote storage\ndvc pull\n</code></pre>"},{"location":"source/data/#common-dvc-issues-and-solutions","title":"Common DVC Issues and Solutions","text":""},{"location":"source/data/#known-problems-encountered","title":"Known Problems Encountered","text":"<ol> <li> <p>Push Command Failures <code>bash    # This command often fails    dvc push --no-cache</code></p> </li> <li> <p>Remote Configuration Issues    ```bash    # Remove problematic remotes    dvc remote remove gcp_storage</p> </li> </ol> <p># Set correct default remote    dvc remote default remote_storage    ```</p> <ol> <li>Authentication Problems</li> <li>Ensure Google Cloud SDK is properly configured</li> <li>Verify bucket permissions and access rights</li> </ol>"},{"location":"source/data/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># List configured remotes\ndvc remote list\n\n# Check remote configuration\ndvc remote list --show-origin\n\n# Verify data status\ndvc status\n\n# Force refresh from remote\ndvc fetch --all-commits\n</code></pre>"},{"location":"source/data/#alternative-data-access","title":"Alternative Data Access","text":"<p>If DVC setup encounters persistent issues, you can:</p> <ol> <li> <p>Direct GCS Access <code>bash    # Access data directly from GCS buckets    gsutil -m cp -r gs://dvc-storage-sensor/data/ ./data/</code></p> </li> <li> <p>Manual Dataset Download</p> </li> <li>Download Food103 segmentation dataset from: https://paperswithcode.com/dataset/foodseg103</li> <li>Extract and place in <code>data/</code> directory</li> </ol>"},{"location":"source/data/#data-module-integration","title":"Data Module Integration","text":"<p>The data module integrates with DVC by: - Automatically checking for DVC-tracked files - Handling both local and remote data sources - Providing fallback mechanisms when DVC is unavailable - Managing data preprocessing pipelines with version control</p>"},{"location":"source/data/#best-practices","title":"Best Practices","text":"<ul> <li>Always verify DVC remote configuration before starting work</li> <li>Use <code>dvc status</code> to check data synchronization</li> <li>Keep <code>.dvc</code> files in version control</li> <li>Test data access in development environment before production deployment</li> </ul> <p>This setup ensures reproducible data management while providing flexibility when DVC encounters configuration issues.</p>"},{"location":"source/main/","title":"Main Pipeline","text":"<p>Main training pipeline with Hydra configuration management for food segmentation.</p>"},{"location":"source/main/#cli-and-hydra-configuration-management","title":"CLI and Hydra Configuration Management","text":"<p>This module demonstrates advanced CLI configuration management using Hydra and OmegaConf, providing flexible parameter override capabilities and structured configuration handling.</p>"},{"location":"source/main/#key-configuration-features","title":"Key Configuration Features","text":"<p>Hydra Integration:</p> <pre><code>@hydra.main(version_base=None, config_path=config_dir, config_name=\"config\")\ndef main(cfg: DictConfig):\n</code></pre> <p>Dynamic Configuration Loading: - Automatically resolves config directory from project root - Loads configuration from <code>configs/config.yaml</code> - Supports hierarchical configuration structures</p>"},{"location":"source/main/#cli-usage-examples","title":"CLI Usage Examples","text":"<p>Default Configuration:</p> <pre><code>python src/segmentation/main.py\n</code></pre> <p>Parameter Override:</p> <pre><code>python src/segmentation/main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001\n</code></pre> <p>Multiple Parameters:</p> <pre><code>python src/segmentation/main.py model.hyperparameters.epochs=100 model.hyperparameters.batch_size=32 paths.base_dir=/custom/path\n</code></pre>"},{"location":"source/main/#configuration-structure","title":"Configuration Structure","text":"<p>The pipeline expects configuration with the following structure:</p> <pre><code>model:\n  hyperparameters:\n    epochs: 20\n    batch_size: 16\n    lr: 0.0001\n\npaths:\n  base_dir: \"data/\"\n\nprofiling:\n  enabled: false\n</code></pre>"},{"location":"source/main/#enhanced-user-experience","title":"Enhanced User Experience","text":"<p>Rich Console Integration: - Displays formatted hyperparameter tables - Provides colorized progress indicators - Shows training status with visual panels</p> <p>Automatic Environment Setup: - Configures Weights &amp; Biases (wandb) in silent mode - Sets up proper Python path resolution - Handles project root discovery</p>"},{"location":"source/main/#pipeline-workflow","title":"Pipeline Workflow","text":"<ol> <li>Configuration Loading - Hydra loads and validates config</li> <li>Parameter Display - Rich table shows current hyperparameters</li> <li>Data Loading - Initializes train/test data loaders</li> <li>Training Execution - Runs complete training pipeline</li> <li>Visualization Generation - Creates metrics and prediction plots</li> <li>Completion - Displays success confirmation</li> </ol> <p>This approach provides reproducible experiments with easy parameter tuning through CLI overrides, making it ideal for hyperparameter sweeps and experiment tracking.</p>"},{"location":"source/main/#src.segmentation.main","title":"<code>src.segmentation.main</code>","text":""},{"location":"source/main/#src.segmentation.main.main","title":"<code>main(cfg)</code>","text":"<p>Main training pipeline for food segmentation model.</p> <p>Orchestrates the complete training workflow including data loading, model training, and visualization generation. Uses Hydra for configuration management and Rich for enhanced console output.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object containing all training parameters, paths, and settings. Expected structure: - cfg.model.hyperparameters: Training hyperparameters - cfg.paths.base_dir: Base directory for data and outputs - cfg.profiling.enabled: Whether to enable performance profiling</p> required Pipeline Steps <ol> <li>Load and display configuration parameters</li> <li>Initialize data loaders for training and testing</li> <li>Create and configure trainer instance</li> <li>Execute training loop</li> <li>Generate training metrics visualizations</li> <li>Generate prediction visualizations</li> </ol> Example <p>Run with default config:</p> <p>python main.py</p> <p>Run with custom parameters:</p> <p>python main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001</p> Source code in <code>src/segmentation/main.py</code> <pre><code>@hydra.main(version_base=None, config_path=config_dir, config_name=\"config\")\ndef main(cfg: DictConfig):\n    \"\"\"\n    Main training pipeline for food segmentation model.\n\n    Orchestrates the complete training workflow including data loading,\n    model training, and visualization generation. Uses Hydra for configuration\n    management and Rich for enhanced console output.\n\n    Args:\n        cfg (DictConfig): Hydra configuration object containing all training\n            parameters, paths, and settings. Expected structure:\n            - cfg.model.hyperparameters: Training hyperparameters\n            - cfg.paths.base_dir: Base directory for data and outputs\n            - cfg.profiling.enabled: Whether to enable performance profiling\n\n    Pipeline Steps:\n        1. Load and display configuration parameters\n        2. Initialize data loaders for training and testing\n        3. Create and configure trainer instance\n        4. Execute training loop\n        5. Generate training metrics visualizations\n        6. Generate prediction visualizations\n\n    Example:\n        Run with default config:\n        &gt;&gt;&gt; python main.py\n\n        Run with custom parameters:\n        &gt;&gt;&gt; python main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001\n    \"\"\"\n    epochs = cfg.model.hyperparameters.epochs\n    batch_size = cfg.model.hyperparameters.batch_size\n    learning_rate = cfg.model.hyperparameters.lr\n    base_dir = cfg.paths.base_dir\n    profiling_enabled = cfg.profiling.enabled\n    print_hyperparameters_table(cfg)\n\n    console.print(\n        Panel.fit(\"\ud83c\udf55 Food Segmentation Training Pipeline\", style=\"bold green\")\n    )\n    console.print()\n\n    console.print(\"[yellow]\ud83d\udcca Loading data...\")\n    train_loader, test_loader = data_loaders(\n        base_dir=base_dir, batch_size=batch_size, num_workers=4\n    )\n    console.print(\n        f\"[green]\u2713 Train: {len(train_loader.dataset)} samples | Test: {len(test_loader.dataset)} samples[/green]\"\n    )\n    console.print()\n\n    console.print(\"[blue]\ud83d\ude80 Starting training...[/blue]\")\n    trainer = Trainer(\n        epochs=epochs,\n        base_dir=base_dir,\n        batch_size=batch_size,\n        lr=learning_rate,\n        enable_profiler=profiling_enabled,\n        init_wandb=True,\n    )\n\n    trainer.train()\n\n    console.print(\"[magenta]\ud83d\udcc8 Generating visualizations...[/magenta]\")\n    trainer.visualize_training_metrics()\n    trainer.visualize_predictions()\n\n    console.print(\n        Panel.fit(\"\u2705 Training Pipeline Completed Successfully!\", style=\"bold green\")\n    )\n</code></pre>"},{"location":"source/main/#src.segmentation.main.print_hyperparameters_table","title":"<code>print_hyperparameters_table(cfg)</code>","text":"<p>Display hyperparameters in a clean table format using Rich.</p> <p>Creates a formatted table showing key training configuration parameters including epochs, batch size, learning rate, and base directory path.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object containing model hyperparameters and path settings. Expected to have structure: - cfg.model.hyperparameters.epochs - cfg.model.hyperparameters.batch_size - cfg.model.hyperparameters.lr - cfg.paths.base_dir</p> required Example <p>print_hyperparameters_table(cfg)</p> Source code in <code>src/segmentation/main.py</code> <pre><code>def print_hyperparameters_table(cfg: DictConfig):\n    \"\"\"\n    Display hyperparameters in a clean table format using Rich.\n\n    Creates a formatted table showing key training configuration parameters\n    including epochs, batch size, learning rate, and base directory path.\n\n    Args:\n        cfg (DictConfig): Hydra configuration object containing model hyperparameters\n            and path settings. Expected to have structure:\n            - cfg.model.hyperparameters.epochs\n            - cfg.model.hyperparameters.batch_size\n            - cfg.model.hyperparameters.lr\n            - cfg.paths.base_dir\n\n    Example:\n        &gt;&gt;&gt; print_hyperparameters_table(cfg)\n        # Displays a formatted table with training configuration\n    \"\"\"\n    table = Table(\n        title=\"\ud83d\udd27 Training Configuration\", show_header=True, header_style=\"bold blue\"\n    )\n    table.add_column(\"Parameter\", style=\"cyan\", width=20)\n    table.add_column(\"Value\", style=\"white\", width=15)\n\n    table.add_row(\"Epochs\", str(cfg.model.hyperparameters.epochs))\n    table.add_row(\"Batch Size\", str(cfg.model.hyperparameters.batch_size))\n    table.add_row(\"Learning Rate\", str(cfg.model.hyperparameters.lr))\n    table.add_row(\"Base Directory\", str(cfg.paths.base_dir))\n\n    console.print(table)\n    console.print()\n</code></pre>"},{"location":"source/main/#src.segmentation.main.print_hyperparameters_table--displays-a-formatted-table-with-training-configuration","title":"Displays a formatted table with training configuration","text":""},{"location":"source/model/","title":"Model Architecture","text":"<p>Implementation of the MiniUNet segmentation model.</p>"},{"location":"source/model/#src.segmentation.model","title":"<code>src.segmentation.model</code>","text":""},{"location":"source/model/#src.segmentation.model.MiniUNet","title":"<code>MiniUNet()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Lightweight U-Net architecture for semantic segmentation.</p> <p>A simplified version of the U-Net architecture designed for food segmentation with 104 output classes. Features encoder-decoder structure with skip connections and proper weight initialization.</p> Architecture <ul> <li>Encoder: 3 conv blocks with max pooling (3\u219264\u2192128\u2192256 channels)</li> <li>Bottleneck: 1 conv block (256\u2192512 channels)</li> <li>Decoder: 3 conv blocks with transpose convolutions (512\u2192256\u2192128\u219264 channels)</li> <li>Output: 1x1 conv to 104 classes</li> </ul> <p>Attributes:</p> Name Type Description <code>encoder1,</code> <code>(encoder2, encoder3)</code> <p>Encoder convolutional blocks</p> <code>bottleneck</code> <p>Bottleneck convolutional block</p> <code>decoder1,</code> <code>(decoder2, decoder3)</code> <p>Decoder convolutional blocks</p> <code>pool</code> <p>Max pooling layer for downsampling</p> <code>upconv1,</code> <code>(upconv2, upconv3)</code> <p>Transpose convolutions for upsampling</p> <code>final</code> <p>Final 1x1 convolution to output classes</p> Example <p>model = MiniUNet() x = torch.rand(1, 3, 224, 224)  # Batch of 1, RGB image output = model(x) print(output.shape)  # torch.Size([1, 104, 224, 224])</p> <p>Initialize the MiniUNet model.</p> <p>Sets up the encoder-decoder architecture with skip connections, pooling/upsampling layers, and applies He weight initialization.</p> Source code in <code>src/segmentation/model.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the MiniUNet model.\n\n    Sets up the encoder-decoder architecture with skip connections,\n    pooling/upsampling layers, and applies He weight initialization.\n    \"\"\"\n    super(MiniUNet, self).__init__()\n\n    # Encoder\n    self.encoder1 = self.conv_block(3, 64)\n    self.encoder2 = self.conv_block(64, 128)\n    self.encoder3 = self.conv_block(128, 256)\n\n    # Bottleneck\n    self.bottleneck = self.conv_block(256, 512)\n\n    # Decoder\n    self.decoder1 = self.conv_block(512, 256)\n    self.decoder2 = self.conv_block(256, 128)\n    self.decoder3 = self.conv_block(128, 64)\n\n    # Pooling and upsampling\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    # Upsampling\n    self.upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n    self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n    self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n\n    # Final layer\n    self.final = nn.Sequential(\n        nn.Conv2d(64, 104, kernel_size=1),\n    )\n\n    # \u2705 Initialize weights\n    self._initialize_weights()\n</code></pre>"},{"location":"source/model/#src.segmentation.model.MiniUNet.conv_block","title":"<code>conv_block(in_channels, out_channels)</code>","text":"<p>Create a convolutional block with two conv layers and ReLU activations.</p> <p>Each block consists of two 3x3 convolutions with padding, followed by ReLU activations. This design increases the receptive field while maintaining spatial dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <p>Returns:</p> Type Description <p>nn.Sequential: Sequential container with conv layers and ReLU activations</p> Note <p>Using two conv layers increases the receptive field and adds non-linearity without increasing parameters significantly.</p> Source code in <code>src/segmentation/model.py</code> <pre><code>def conv_block(self, in_channels, out_channels):\n    \"\"\"\n    Create a convolutional block with two conv layers and ReLU activations.\n\n    Each block consists of two 3x3 convolutions with padding, followed by\n    ReLU activations. This design increases the receptive field while\n    maintaining spatial dimensions.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n\n    Returns:\n        nn.Sequential: Sequential container with conv layers and ReLU activations\n\n    Note:\n        Using two conv layers increases the receptive field and adds\n        non-linearity without increasing parameters significantly.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n    )\n</code></pre>"},{"location":"source/model/#src.segmentation.model.MiniUNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the MiniUNet model.</p> <p>Implements the U-Net architecture with encoder-decoder structure and skip connections. The encoder progressively reduces spatial dimensions while increasing channel depth. The decoder upsamples and combines features using skip connections.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor with shape (B, 3, H, W) where: - B: batch size - 3: RGB channels - H, W: height and width</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Segmentation logits with shape (B, 104, H, W) where: - B: batch size - 104: number of food classes - H, W: same as input dimensions</p> Architecture Flow <ol> <li>Encoder: x \u2192 enc1 \u2192 enc2 \u2192 enc3</li> <li>Bottleneck: enc3 \u2192 bottleneck</li> <li>Decoder: bottleneck + enc3 \u2192 dec1 + enc2 \u2192 dec2 + enc1 \u2192 dec3</li> <li>Output: dec3 \u2192 final (104 classes)</li> </ol> Source code in <code>src/segmentation/model.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass through the MiniUNet model.\n\n    Implements the U-Net architecture with encoder-decoder structure\n    and skip connections. The encoder progressively reduces spatial\n    dimensions while increasing channel depth. The decoder upsamples\n    and combines features using skip connections.\n\n    Args:\n        x (torch.Tensor): Input tensor with shape (B, 3, H, W) where:\n            - B: batch size\n            - 3: RGB channels\n            - H, W: height and width\n\n    Returns:\n        torch.Tensor: Segmentation logits with shape (B, 104, H, W) where:\n            - B: batch size\n            - 104: number of food classes\n            - H, W: same as input dimensions\n\n    Architecture Flow:\n        1. Encoder: x \u2192 enc1 \u2192 enc2 \u2192 enc3\n        2. Bottleneck: enc3 \u2192 bottleneck\n        3. Decoder: bottleneck + enc3 \u2192 dec1 + enc2 \u2192 dec2 + enc1 \u2192 dec3\n        4. Output: dec3 \u2192 final (104 classes)\n    \"\"\"\n    # Encoder\n    enc1 = self.encoder1(x)\n    enc2 = self.encoder2(self.pool(enc1))\n    enc3 = self.encoder3(self.pool(enc2))\n\n    # Bottleneck\n    bottleneck = self.bottleneck(self.pool(enc3))\n\n    # Decoder with skip connections\n    dec1 = self.decoder1(torch.cat([self.upconv1(bottleneck), enc3], dim=1))\n    dec2 = self.decoder2(torch.cat([self.upconv2(dec1), enc2], dim=1))\n    dec3 = self.decoder3(torch.cat([self.upconv3(dec2), enc1], dim=1))\n\n    return self.final(dec3)\n</code></pre>"},{"location":"source/training/","title":"Training Module","text":"<p>Comprehensive training pipeline with metrics tracking and visualization for food segmentation.</p>"},{"location":"source/training/#what-we-are-tracking","title":"What We Are Tracking","text":""},{"location":"source/training/#core-metrics","title":"Core Metrics","text":"<ul> <li>Loss Functions - Training and validation loss per epoch</li> <li>Segmentation Accuracy - Pixel-wise accuracy and mean IoU</li> <li>Learning Progress - Learning rate schedules and training time</li> <li>Model Performance - Validation metrics and best model checkpointing</li> </ul>"},{"location":"source/training/#experiment-tracking","title":"Experiment Tracking","text":"<ul> <li>Weights &amp; Biases Integration - Hyperparameters, model architecture, and system metrics</li> <li>Visualization Outputs - Training curves, loss plots, and prediction visualizations</li> </ul>"},{"location":"source/training/#tracking-architecture","title":"Tracking Architecture","text":"<p>Hybrid approach combining: - Local logging with Rich console output - Weights &amp; Biases for cloud-based experiment tracking - File-based visualization saves - Model checkpoint management</p> <p>This focuses on essential segmentation metrics while maintaining training pipeline simplicity.</p>"},{"location":"source/training/#src.segmentation.train","title":"<code>src.segmentation.train</code>","text":""},{"location":"source/training/#src.segmentation.train.Trainer","title":"<code>Trainer(lr=None, epochs=None, batch_size=None, base_dir=None, enable_profiler=None, init_wandb=True, prune_amount=0.2)</code>","text":"<p>Initialize the Trainer and do experimental logging with Weights and Biases.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>None</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>None</code> <code>base_dir</code> <code>str</code> <p>Project's root directory.</p> <code>None</code> <code>enable_profiler</code> <code>bool</code> <p>Whether to enable PyTorch profiler.</p> <code>None</code> <code>init_wandb</code> <code>bool</code> <p>Whether to initialize Weights and Biases.</p> <code>True</code> Source code in <code>src/segmentation/train.py</code> <pre><code>def __init__(\n    self,\n    lr=None,\n    epochs=None,\n    batch_size=None,\n    base_dir=None,\n    enable_profiler=None,\n    init_wandb=True,\n    prune_amount=0.2,\n):\n    \"\"\"\n    Initialize the Trainer and do experimental logging with Weights and Biases.\n\n    Args:\n        lr (float): Learning rate for the optimizer.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        base_dir (str): Project's root directory.\n        enable_profiler (bool): Whether to enable PyTorch profiler.\n        init_wandb (bool): Whether to initialize Weights and Biases.\n    \"\"\"\n\n    super().__init__()\n\n    # model\n    self.model = MiniUNet().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Check if pruning is enabled\n    if prune_amount &gt; 0.0:\n        logger.info(f\"Applying pruning with amount: {prune_amount}\")\n\n        # Apply global unstructured pruning to Conv2d and Linear layers\n        parameters_to_prune = []\n        for module in self.model.modules():\n            if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n                parameters_to_prune.append((module, \"weight\"))\n\n        if parameters_to_prune:\n            prune.global_unstructured(\n                parameters_to_prune,\n                pruning_method=prune.L1Unstructured,\n                amount=prune_amount,\n            )\n            logger.info(\"Pruning applied to the model.\")\n        else:\n            logger.warning(\"No Conv2d or Linear layers found for pruning.\")\n\n    # Model Parameters\n    self.parameters = sum(\n        p.numel() for p in self.model.parameters() if p.requires_grad\n    )\n\n    # Training Parameters\n    self.epochs = epochs\n    self.lr = lr\n    self.batch_size = batch_size\n\n    # Directories\n    self.base_dir = base_dir\n    self.saved_dir = os.path.join(self.base_dir, \"saved\")\n    self.model_path = os.path.join(self.saved_dir, \"models\", \"model.pth\")\n    self.plots_path = os.path.join(\n        self.saved_dir, \"reports\", \"training_metrics.png\"\n    )\n    self.predictions = os.path.join(\n        self.saved_dir, \"predictions\", \"predictions.png\"\n    )\n    # Data Loaders\n    self.train_loader, self.test_loader = data_loaders(\n        base_dir=self.base_dir,\n        batch_size=self.batch_size,\n    )\n\n    # Loss function\n    self.loss = nn.CrossEntropyLoss()\n    self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n\n    # Training History\n    self.train_losses = []\n    self.test_losses = []\n    self.train_accs = []\n    self.test_accs = []\n    self.train_ious = []\n    self.test_ious = []\n    self.best_test_loss = float(\"inf\")\n\n    logger.info(f\"Model initialized with {self.parameters} trainable parameters.\")\n\n    self.init_wandb = init_wandb\n    self.enable_profiler = enable_profiler\n    if not self.enable_profiler:\n        logger.info(\"Profiler is disabled. Training will run without profiling.\")\n\n    # Initialize Weights and Biases\n    if self.init_wandb and wandb.run is None:\n        wandb.init(\n            project=\"Food-Segmentation\",\n            config={\n                \"epochs\": self.epochs,\n                \"learning_rate\": self.lr,\n                \"batch_size\": self.batch_size,\n                \"base_dir\": self.base_dir,\n                \"trainable_params\": self.parameters,\n                \"model\": \"MiniUNet\",\n                \"optimizer\": \"Adam\",\n                \"loss_function\": \"CrossEntropyLoss\",\n            },\n        )\n        wandb.watch(self.model, log=\"all\")\n        logger.info(\"Weights and Biases initialized for tracking.\")\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.calculate_iou","title":"<code>calculate_iou(pred_mask, true_mask, num_classes=104)</code>","text":"<p>Implements the Intersection over Union (IoU) metric for segmentation tasks.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def calculate_iou(self, pred_mask, true_mask, num_classes=104):\n    \"\"\"Implements the Intersection over Union (IoU) metric for segmentation tasks.\"\"\"\n    ious = []\n    pred_mask = pred_mask.view(-1)\n    true_mask = true_mask.view(-1)\n\n    for cls in range(num_classes):\n        pred_inds = pred_mask == cls\n        target_inds = true_mask == cls\n\n        intersection = (pred_inds &amp; target_inds).long().sum().item()\n        union = (pred_inds | target_inds).long().sum().item()\n\n        if union == 0:\n            continue  # Skip if no pixels for this class\n\n        iou = intersection / union\n        ious.append(iou)\n\n    return np.mean(ious) if ious else 0.0\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the model.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass through the model.\"\"\"\n    return self.model(x)\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.remove_pruning","title":"<code>remove_pruning()</code>","text":"<p>Remove pruning from the model.</p> <p>This method iterates through all modules in the model and removes pruning parameters if they exist. It is useful for restoring the original model state after pruning has been applied.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def remove_pruning(self):\n    \"\"\"\n    Remove pruning from the model.\n\n    This method iterates through all modules in the model and removes\n    pruning parameters if they exist. It is useful for restoring the\n    original model state after pruning has been applied.\n    \"\"\"\n\n    for module in self.model.modules():\n        if hasattr(module, \"weight_orig\"):\n            prune.remove(module, \"weight\")\n        if hasattr(module, \"bias_orig\"):\n            prune.remove(module, \"bias\")\n            logger.info(f\"Removed pruning from bias of {module}\")\n\n    logger.info(\"Pruning removed from the model.\")\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.train","title":"<code>train()</code>","text":"<p>Execute the training loop with optional profiling and Weights and Biases logging.</p> <p>Pipeline Steps : 1. Initialize the device for training (GPU or CPU). 2. Check if the model path is set. 3. Create profiler if enabled. 4. Loop through the number of epochs: 5. Train the model on the training dataset. 6. Calculate and log training metrics (loss, accuracy, IoU). 7. Validate the model on the test dataset. 8. Save the best model based on test loss. 9. Log metrics to Weights and Biases. 10. Finish Weights and Biases run.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def train(self):\n    \"\"\"\n\n    Execute the training loop with optional profiling and Weights and Biases logging.\n\n    Pipeline Steps :\n    1. Initialize the device for training (GPU or CPU).\n    2. Check if the model path is set.\n    3. Create profiler if enabled.\n    4. Loop through the number of epochs:\n    5. Train the model on the training dataset.\n    6. Calculate and log training metrics (loss, accuracy, IoU).\n    7. Validate the model on the test dataset.\n    8. Save the best model based on test loss.\n    9. Log metrics to Weights and Biases.\n    10. Finish Weights and Biases run.\n\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if not self.model_path:\n        logger.warning(\"Model path is not set. Cannot save the model.\")\n        return\n\n    os.makedirs(\"./profiler_logs\", exist_ok=True)\n\n    prof = None\n\n    if self.enable_profiler:\n        prof = profile(\n            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n            schedule=schedule(wait=1, warmup=1, active=3, repeat=2),\n            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n                \"./profiler_logs\"\n            ),\n            record_shapes=True,\n            profile_memory=True,\n            with_stack=True,\n        )\n        prof.start()\n\n    for epoch in range(self.epochs):\n        # Training\n        self.model.train()\n\n        running_loss = 0.0\n        running_accuracy = 0.0\n        running_iou = 0.0\n\n        train_bar = tqdm(\n            self.train_loader,\n            desc=f\"Epoch {epoch+1}/{self.epochs} [Train]\",\n            leave=False,\n        )\n\n        for batch_idx, (images, masks) in enumerate(train_bar):\n            images = images.to(device)\n            masks = masks.to(device).long()\n\n            self.optimizer.zero_grad()\n            outputs = self.forward(images)\n            loss = self.loss(outputs, masks)\n            loss.backward()\n            self.optimizer.step()\n\n            # Calculate accuracy for current batch\n            pred_classes = torch.argmax(outputs, dim=1)\n            train_accuracy = torch.mean((pred_classes == masks).float())\n            train_iou = self.calculate_iou(pred_classes, masks)\n\n            # Accumulate loss and accuracy\n            running_accuracy += train_accuracy.item() * images.size(0)\n            running_loss += loss.item() * images.size(0)\n            running_iou += train_iou * images.size(0)\n\n            # Update progress bar\n            train_bar.set_postfix(loss=loss.item())\n\n            # Step profiler and limit batches for profiling\n            if self.enable_profiler and prof is not None:\n                prof.step()\n                logger.info(f\"Profiler step {batch_idx + 1}\")\n                if batch_idx &gt;= 20:\n                    logger.info(\"\ud83d\udd25 Profiling completed\")\n                    prof.stop()\n                    break\n\n        # Training metrics\n        train_loss = running_loss / len(self.train_loader.dataset)\n        train_acc = running_accuracy / len(self.train_loader.dataset)\n        train_iou = running_iou / len(self.train_loader.dataset)\n        self.train_losses.append(train_loss)\n        self.train_accs.append(train_acc)\n        self.train_ious.append(train_iou)\n\n        # Your existing validation code...\n        self.model.eval()\n        running_loss = 0.0\n        running_accuracy = 0.0\n        running_iou = 0.0\n\n        test_bar = tqdm(\n            self.test_loader,\n            desc=f\"Epoch {epoch+1}/{self.epochs} [Test]\",\n            leave=False,\n        )\n\n        with torch.no_grad():\n            for images, masks in test_bar:\n                images = images.to(device)\n                masks = masks.to(device).long()\n\n                outputs = self.forward(images)\n                loss = self.loss(outputs, masks)\n\n                pred_classes = torch.argmax(outputs, dim=1)\n                test_accuracy = torch.mean((pred_classes == masks).float())\n                test_iou = self.calculate_iou(pred_classes, masks)\n\n                running_accuracy += test_accuracy.item() * images.size(0)\n                running_loss += loss.item() * images.size(0)\n                running_iou += test_iou * images.size(0)\n\n                test_bar.set_postfix(test_loss=loss.item())\n\n        test_loss = running_loss / len(self.test_loader.dataset)\n        test_acc = running_accuracy / len(self.test_loader.dataset)\n        test_iou = running_iou / len(self.test_loader.dataset)\n\n        self.test_losses.append(test_loss)\n        self.test_accs.append(test_acc)\n        self.test_ious.append(test_iou)\n\n        print(f\"Epoch [{epoch+1}/{self.epochs}]:\")\n        print(\n            f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train IoU: {train_iou:.4f}\"\n        )\n        print(\n            f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test IoU: {test_iou:.4f}\"\n        )\n\n        wandb.log(\n            {\n                \"Train Loss\": train_loss,\n                \"Train Accuracy\": train_acc,\n                \"Test Loss\": test_loss,\n                \"Test Accuracy\": test_acc,\n                \"epoch\": epoch + 1,\n                \"Train IoU\": train_iou,\n                \"Test IoU\": test_iou,\n            }\n        )\n\n        if test_loss &lt; self.best_test_loss:\n            self.best_test_loss = test_loss\n            self.remove_pruning()\n            torch.save(\n                {\n                    \"model_state_dict\": self.model.state_dict(),\n                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n                    \"epoch\": epoch + 1,\n                    \"best_test_loss\": self.best_test_loss,\n                    \"final_train_loss\": train_loss,\n                    \"final_test_loss\": test_loss,\n                    \"final_train_acc\": train_acc,\n                    \"final_test_acc\": test_acc,\n                },\n                self.model_path,\n            )\n            logger.info(f\"Best model saved with test_loss: {test_loss:.4f}\")\n\n            artifact = wandb.Artifact(\n                \"model\",\n                type=\"model\",\n                description=\"Best model based on test loss\",\n            )\n            artifact.add_file(self.model_path)\n            wandb.log_artifact(artifact)\n\n        logger.info(f\"Training complete. Model saved at {self.model_path}\")\n        print(\"Training complete.\")\n\n    wandb.finish()\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.visualize_training_metrics","title":"<code>visualize_training_metrics()</code>","text":"<p>Visualize training and testing loss &amp; accuracy from saved model checkpoint Creates two side-by-side graphs: Loss comparison and Accuracy comparison</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <p>Base directory for saving plots</p> required <code>model_path</code> <p>Path to the saved model checkpoint (.pth file)</p> required <code>plots_path</code> <p>Directory to save the plots</p> required Source code in <code>src/segmentation/train.py</code> <pre><code>def visualize_training_metrics(self):\n    \"\"\"\n    Visualize training and testing loss &amp; accuracy from saved model checkpoint\n    Creates two side-by-side graphs: Loss comparison and Accuracy comparison\n\n    Args:\n        base_dir: Base directory for saving plots\n        model_path: Path to the saved model checkpoint (.pth file)\n        plots_path: Directory to save the plots\n    \"\"\"\n\n    # Extract metrics\n    train_losses = self.train_losses\n    test_losses = self.test_losses\n    train_accs = self.train_accs\n    test_accs = self.test_accs\n    train_ious = self.train_ious\n    test_ious = self.test_ious\n\n    # Check if data exists\n    if not train_losses or not test_losses:\n        logger.warning(\"No loss data found in checkpoint!\")\n        return\n\n    if not train_accs or not test_accs:\n        logger.warning(\"No accuracy data found in checkpoint!\")\n        return\n\n    if not self.plots_path:\n        logger.warning(\"No plots path provided. Plot not saved.\")\n        return\n\n    # Create figure with 3 subplots side by side\n    _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n\n    epochs = range(1, len(train_losses) + 1)\n\n    # Graph 1: Training vs Testing Loss\n    ax1.plot(\n        epochs,\n        train_losses,\n        label=\"Training Loss\",\n        color=\"blue\",\n        marker=\"o\",\n        linewidth=2,\n    )\n    ax1.plot(\n        epochs,\n        test_losses,\n        label=\"Testing Loss\",\n        color=\"red\",\n        marker=\"s\",\n        linewidth=2,\n    )\n    ax1.set_title(\"Loss Comparison\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n    ax1.grid(True)\n\n    # Graph 2: Training vs Testing Accuracy\n    ax2.plot(\n        epochs,\n        train_accs,\n        label=\"Training Accuracy\",\n        color=\"green\",\n        marker=\"o\",\n        linewidth=2,\n    )\n    ax2.plot(\n        epochs,\n        test_accs,\n        label=\"Testing Accuracy\",\n        color=\"orange\",\n        marker=\"s\",\n        linewidth=2,\n    )\n    ax2.set_title(\"Accuracy Comparison\")\n    ax2.set_xlabel(\"Epochs\")\n    ax2.set_ylabel(\"Accuracy\")\n    ax2.legend()\n    ax2.grid(True)\n\n    # Graph 3: Training vs Testing IoU\n    ax3.plot(\n        epochs,\n        train_ious,\n        label=\"Training IoU\",\n        color=\"purple\",\n        marker=\"o\",\n        linewidth=2,\n    )\n    ax3.plot(\n        epochs,\n        test_ious,\n        label=\"Testing IoU\",\n        color=\"brown\",\n        marker=\"s\",\n        linewidth=2,\n    )\n    ax3.set_title(\"IoU Comparison\")\n    ax3.set_xlabel(\"Epochs\")\n    ax3.set_ylabel(\"IoU\")\n    ax3.legend()\n    ax3.grid(True)\n\n    # Adjust layout and save the plot\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.85)  # Adjust top to make space for the title\n    plt.suptitle(\"Training and Testing Metrics\", fontsize=20)\n    plt.savefig(self.plots_path, dpi=300)  # Save the plot with high resolution\n    logger.info(f\"Training metrics plot saved: {self.plots_path}\")\n    plt.tight_layout()\n    plt.show()  # Show the plot in interactive mode\n    plt.close()  # Close the plot to free memory\n</code></pre>"}]}