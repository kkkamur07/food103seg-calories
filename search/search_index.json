{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Food103Seg Calories","text":"<p>Welcome to Food103Seg - a deep learning project for food image segmentation.</p>"},{"location":"#live-demo","title":"\ud83d\ude80 Live Demo","text":"<p>Try the application now! Our Streamlit app is live at: the link</p>"},{"location":"#overview","title":"Overview","text":"<p>This project uses computer vision to identify different food items in images by segmenting them. We have Built this with PyTorch and by using a simplified MiniUNet architecture, it can segment 104 different food categories with moderate accuracy. We trained the MiniUNet architecture on RTX 4090 for about 20 Minutes and the hyperparameter configuration were determined by the hyperparameter sweep of wandb. </p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udf55 Food Segmentation: Identifies and segments different food items in images</li> <li>\ud83e\udde0 MiniUNet Model: Lightweight U-Net architecture optimized for food segmentation</li> <li>\ud83c\udf10 Web Interface: User-friendly Streamlit application</li> <li>\u2699\ufe0f Easy Configuration: Hydra-based configuration management</li> <li>\ud83d\udcc8 Experiment Tracking: Integration with Weights &amp; Biases</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/kkkamur07/food103seg-calories\n\n# Navigate to the project directory\ncd food103seg-calories\n\n# Create virtual environment using UV\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install the dependencies \nuv pip install -r requirements.txt\n\n# Train the model\npython -m src.segmentation.main\n\n# Launch API Server\nuvicorn src.app.api:app --host 0.0.0.0 --port 8000 --reload\n\n# Launch Web App\nstreamlit run src/app/frontend.py\n</code></pre>"},{"location":"#mlops-template","title":"MLOps Template","text":"<p>We provide a ready-to-use MLOps template for this project structure:</p> <pre><code># Install cookiecutter\npip install cookiecutter\n\n# Generate project using our template\ncookiecutter https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Find the complete template and installation guide at:</p> <pre><code>https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Sources</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 segmentation/         # Core ML modules\n\u2502   \u2502   \u251c\u2500\u2500 data.py           # Data loading\n\u2502   \u2502   \u251c\u2500\u2500 model.py          # MiniUNet architecture\n\u2502   \u2502   \u251c\u2500\u2500 train.py          # Training pipeline\n\u2502   \u2502   \u2514\u2500\u2500 main.py           # Main training script\n\u2502   \u2514\u2500\u2500 app/                  # Web application\n\u2502       \u251c\u2500\u2500 frontend.py       # Streamlit interface\n\u2502       \u2514\u2500\u2500 service.py        # API service\n\u251c\u2500\u2500 configs/                  # Configuration files and management\n\u251c\u2500\u2500 notebooks/                # Jupyter notebooks for experiments\n\u2514\u2500\u2500 docs/                     # Documentation\n</code></pre> <p>We have also used <code>bentoML</code> for optimized API for ML models specially, this can be found in. </p> <pre><code>\u251c\u2500\u2500 src/\n    \u251c\u2500\u2500 segmentation/ \n        \u251c\u2500\u2500 bentoml.py        # For BentoML\n        \u251c\u2500\u2500 bentoml_setup.py  # For setting up BentoML\n\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Installation - Set up the project</li> <li>Quick Start - Get running quickly</li> <li>API Reference - Explore the code</li> <li>Training Guide - Train your own models</li> </ol>"},{"location":"#model-performance","title":"Model Performance","text":"<ul> <li>104 Food Classes: Comprehensive food category coverage</li> <li>Moderate Accuracy: &gt;20% mean IoU on test set</li> <li>Moderate Inference Speed: ~100ms per image on GPU</li> <li>Lightweight: ~15MB model size</li> </ul> <p>Ready to start? Check out our installation guide or try the live demo!</p> <p>Sources</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the Food103Seg Calories project, ensure you have the following prerequisites:</p> <ul> <li>Python 3.8+ (Python 3.9 or 3.10 recommended)</li> <li>CUDA-compatible GPU (recommended for training, optional for inference)</li> <li>Git for cloning the repository</li> <li>pip package manager</li> </ul>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"Component Minimum Recommended Python 3.8+ 3.9 or 3.10 RAM 8GB 16GB+ GPU Memory 4GB 8GB+ Storage 10GB 20GB+"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#setup-instructions-using-uv","title":"Setup Instructions Using uv","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/kkkamur07/food103seg-calories\ncd food103seg-calories\n</code></pre>"},{"location":"installation/#2-create-virtual-environment-and-install-dependencies","title":"2. Create Virtual Environment and Install Dependencies","text":"<p>Using uv (recommended for fastest setup):</p> <pre><code># Create virtual environment\nuv venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install production dependencies\nuv pip install -r requirements.txt\n\n# Install development dependencies (optional)\nuv pip install -r requirements_dev.txt\n\n# Install project in development mode\nuv pip install -e .\n</code></pre> <p>Alternative one-liner approach:</p> <pre><code># Create environment and install dependencies in one step\nuv venv &amp;&amp; source .venv/bin/activate &amp;&amp; uv pip install -r requirements.txt\n</code></pre>"},{"location":"installation/#3-gpu-setup-optional-but-recommended","title":"3. GPU Setup (Optional but Recommended)","text":"<p>For CUDA support, install PyTorch with CUDA using uv:</p> <pre><code># For CUDA 11.8\nuv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# For CUDA 12.1\nuv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>Verify GPU installation:</p> <pre><code>python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"installation/#about-the-requirements-files","title":"About the Requirements Files","text":"<p>This project provides two dependency files for different use cases[1][2]:</p> <ul> <li><code>requirements.txt</code> - Contains production dependencies needed to run the application</li> <li><code>requirements_dev.txt</code> - Contains additional development dependencies for testing, linting, and development tools[3]</li> </ul> <p>The uv package manager provides significant speed improvements over traditional pip installations[4][2], making it ideal for projects with multiple dependencies. You can install from either file using <code>uv pip install -r &lt;filename&gt;</code>[5].</p>"},{"location":"installation/#data-setup","title":"Data Setup","text":""},{"location":"installation/#1-download-dataset","title":"1. Download Dataset","text":""},{"location":"installation/#data-storage-and-versioning-with-dvc","title":"Data Storage and Versioning with DVC","text":"<p>This project uses DVC (Data Version Control) with Google Cloud Storage for data versioning and management. The data and models are stored in two separate GCS buckets:</p> <ul> <li>Data storage: <code>gs://dvc-storage-sensor/</code></li> <li>Model storage: <code>gs://food-segmentation-models/</code></li> </ul>"},{"location":"installation/#setting-up-dvc-with-google-cloud-storage","title":"Setting Up DVC with Google Cloud Storage","text":"<pre><code># Create data directory\nmkdir -p data\n\n# Install required tools\npip install dvc-gs\n\n# List available GCS buckets\ngsutil ls\n\n# Add remote storage (replace &lt;output-from-gsutils&gt; with actual bucket path)\ndvc remote add -d remote_storage gs://dvc-storage-sensor/\n\n# Configure version-aware storage\ndvc remote modify remote_storage version_aware true\n\n# List configured remotes\ndvc remote list\n\n# Pull data from remote storage\ndvc pull\n</code></pre>"},{"location":"installation/#dvc-management-commands","title":"DVC Management Commands","text":"<pre><code># Remove a remote if needed\ndvc remote remove gcp_storage\n\n# Set default remote\ndvc remote default remote_storage\n\n# Push data (note: --no-cache may have issues)\ndvc push --no-cache\n</code></pre>"},{"location":"installation/#known-issues-with-dvc-setup","title":"Known Issues with DVC Setup","text":"<p>During development, several challenges were encountered with the DVC workflow, particularly with the <code>dvc push --no-cache</code> command. While DVC provides excellent data versioning capabilities, the setup proved complex for this project's requirements.</p>"},{"location":"installation/#alternative-direct-dataset-download","title":"Alternative: Direct Dataset Download","text":"<p>If you prefer to bypass the DVC setup or encounter issues, you can download the Food103 segmentation dataset directly from:</p> <p>Dataset source: https://paperswithcode.com/dataset/foodseg103</p> <pre><code># Create data directory\nmkdir -p data\n\n# Download dataset manually from Papers with Code\n# Extract and place in data/ directory\n</code></pre>"},{"location":"installation/#recommended-approach","title":"Recommended Approach","text":"<p>For this project, you can choose either approach:</p> <ol> <li>DVC approach - Use the GCS buckets with DVC for version control</li> <li>Direct download - Download the Food103 dataset directly from Papers with Code</li> </ol> <p>The DVC setup provides better data versioning and collaboration features, while the direct download approach is simpler for getting started quickly.</p>"},{"location":"installation/#2-expected-data-directory-structure","title":"2. Expected Data Directory Structure","text":"<p>Ensure your data follows this structure:</p> <pre><code>data/\n\u251c\u2500\u2500 Images/\n\u2502   \u251c\u2500\u2500 img_dir/\n\u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 test/\n\u2502   \u2502       \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502       \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ann_dir/\n\u2502       \u251c\u2500\u2500 train/\n\u2502       \u2502   \u251c\u2500\u2500 image1.png\n\u2502       \u2502   \u251c\u2500\u2500 image2.png\n\u2502       \u2502   \u2514\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 test/\n\u2502           \u251c\u2500\u2500 image1.png\n\u2502           \u251c\u2500\u2500 image2.png\n\u2502           \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"installation/#configuration-setup","title":"Configuration Setup","text":""},{"location":"installation/#copy-the-template","title":"Copy the Template","text":"<pre><code># Install cookiecutter\npip install cookiecutter\n\n# Generate project using our template\ncookiecutter https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Find the complete template and installation guide at:</p> <pre><code>https://github.com/kkkamur07/cookie-cutter --directory=mlops\n</code></pre> <p>Sources</p>"},{"location":"installation/#verification","title":"Verification","text":""},{"location":"installation/#1-test-installation","title":"1. Test Installation","text":"<p>Run the following commands to verify your installation:</p> <pre><code># Test imports\npython -c \"import torch; import torchvision; print('PyTorch installed successfully')\"\n\n# Test project modules\npython -c \"from src.segmentation.data import data_loaders; print('Project modules working')\"\n\n# Test data loading\npython -c \"from src.segmentation.data import data_loaders; print('Data loading test passed')\"\n</code></pre>"},{"location":"installation/#2-quick-training-test","title":"2. Quick Training Test","text":"<p>Run a quick training test with minimal epochs:</p> <pre><code>python src/segmentation/main.py model.hyperparameters.epochs=1\n</code></pre>"},{"location":"installation/#running-the-application","title":"Running the Application","text":""},{"location":"installation/#1-streamlit-web-app","title":"1. Streamlit Web App","text":"<pre><code>streamlit run src/app/frontend.py\n</code></pre>"},{"location":"installation/#2-api-server-fastapi-with-uvicorn","title":"2. API Server (FastAPI with Uvicorn)","text":"<pre><code>uvicorn src.app.api:app --host 0.0.0.0 --port 8000 --reload\n</code></pre>"},{"location":"installation/#3-training-pipeline","title":"3. Training Pipeline","text":"<pre><code>python src/segmentation/main.py\n</code></pre>"},{"location":"installation/#4-custom-training","title":"4. Custom Training","text":"<pre><code>python src/segmentation/main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001\n</code></pre>"},{"location":"installation/#access-points","title":"Access Points","text":"<ul> <li>Web App: <code>http://localhost:8501</code></li> <li>API: <code>http://localhost:8000</code></li> <li>API Docs: <code>http://localhost:8000/docs</code></li> </ul>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":""},{"location":"installation/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<ul> <li>Reduce batch size in config: <code>model.hyperparameters.batch_size=16</code></li> </ul>"},{"location":"installation/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code>pip install --upgrade pip\npip install -r requirements.txt --force-reinstall\n</code></pre>"},{"location":"installation/#data-loading-errors","title":"Data Loading Errors","text":"<ul> <li>Verify directory structure matches expected format</li> <li>Check file permissions: <code>chmod -R 755 data/</code></li> <li>Ensure image and annotation files have correct extensions</li> </ul>"},{"location":"installation/#import-errors","title":"Import Errors","text":"<pre><code># Reinstall in development mode\npip install -e . --force-reinstall\n</code></pre>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the logs in <code>saved/logs/</code></li> <li>Verify GPU setup with <code>nvidia-smi</code></li> <li>Review configuration in <code>configs/config.yaml</code></li> <li>Check Python version compatibility</li> </ol>"},{"location":"installation/#optional-components","title":"Optional Components","text":""},{"location":"installation/#docker-setup","title":"Docker Setup","text":"<p>If you prefer Docker:</p> <pre><code># Build backend\ndocker build -f Dockerfile.backend -t food-seg-backend .\n\n# Build frontend\ndocker build -f Dockerfile.frontend -t food-seg-frontend .\n\n# Run with docker-compose\ndocker-compose up\n</code></pre>"},{"location":"installation/#development-tools","title":"Development Tools","text":"<p>Install additional development tools:</p> <pre><code># Pre-commit hooks\npre-commit install\n\n# Jupyter for notebooks\npip install jupyter\njupyter notebook notebooks/\n\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Review the configuration in <code>configs/config.yaml</code></li> <li>Run the training pipeline with your data</li> <li>Explore the Streamlit app at the live link above</li> <li>Check the documentation for advanced usage</li> <li>Set up monitoring with Weights &amp; Biases</li> </ol> <p>You're now ready to start training your food segmentation model and estimating calories! \ud83c\udf55\ud83d\udcca</p>"},{"location":"structure/","title":"Project Structure","text":"<pre><code>kkkamur07-food103seg-calories/\n\u251c\u2500\u2500 README.md                          # Project overview and instructions\n\u251c\u2500\u2500 cloudbuild.yaml                    # Google Cloud Build config\n\u251c\u2500\u2500 data.dvc                           # DVC-tracked data file\n\u251c\u2500\u2500 docker-compose.yml                 # Orchestration of backend + frontend\n\u251c\u2500\u2500 Dockerfile.backend                 # Dockerfile for backend service\n\u251c\u2500\u2500 Dockerfile.frontend                # Dockerfile for frontend app\n\u251c\u2500\u2500 pyproject.toml                     # Python project metadata + build system\n\u251c\u2500\u2500 requirements.txt                   # Production dependencies\n\u251c\u2500\u2500 requirements_dev.txt               # Dev dependencies (linting, testing)\n\u251c\u2500\u2500 tasks.py                           # Automation scripts (e.g. via `invoke`)\n\u251c\u2500\u2500 uv.lock                            # Dependency lock file for uv tool\n\u251c\u2500\u2500 wandb_runner.py                    # W&amp;B experiment runner for hyperparameter sweep\n\u251c\u2500\u2500 .dockerignore                      # Ignore rules for Docker builds\n\u251c\u2500\u2500 .dvcignore                         # Ignore rules for DVC\n\u251c\u2500\u2500 .pre-commit-config.yaml            # Pre-commit hooks config\n\u251c\u2500\u2500 configs/                           # All project configs\n\u2502   \u251c\u2500\u2500 config.yaml                    # Main config file (training, paths)\n\u2502   \u251c\u2500\u2500 wandb_sweep.yaml               # W&amp;B sweep configuration\n\u2502   \u251c\u2500\u2500 dataset/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml               # Dataset-specific config\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml               # Model-specific config\n\u2502   \u2514\u2500\u2500 outputs/                       # Experiment outputs\n\u2502       \u251c\u2500\u2500 2025-07-02/\n\u2502       \u2502   \u2514\u2500\u2500 22-43-00/\n\u2502       \u2502       \u251c\u2500\u2500 wandb/             # W&amp;B run logs\n\u2502       \u2502       \u2514\u2500\u2500 .hydra/            # Hydra config snapshots\n\u2502       \u2502           \u251c\u2500\u2500 config.yaml\n\u2502       \u2502           \u2514\u2500\u2500 hydra.yaml\n\u2502       \u2514\u2500\u2500 2025-07-04/\n\u2502           \u2514\u2500\u2500 21-10-21/\n\u2502               \u2514\u2500\u2500 .hydra/\n\u2502                   \u2514\u2500\u2500 hydra.yaml\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 experiment.ipynb              # Jupyter notebook for experiments\n\u251c\u2500\u2500 saved/\n\u2502   \u2514\u2500\u2500 models.dvc                    # Tracked model weights &amp; biases file(s) with DVC\n\u251c\u2500\u2500 src/                              # Source code\n\u2502   \u251c\u2500\u2500 app/                          # Application code (serving)\n\u2502   \u2502   \u251c\u2500\u2500 bentoml.py                # BentoML service definition\n\u2502   \u2502   \u251c\u2500\u2500 bentoml_setup.py          # BentoML setup utility\n\u2502   \u2502   \u251c\u2500\u2500 frontend.py               # Streamlit or Gradio frontend\n\u2502   \u2502   \u251c\u2500\u2500 frontend_requirements.txt\n\u2502   \u2502   \u2514\u2500\u2500 service.py                # Service logic\n\u2502   \u251c\u2500\u2500 segmentation/                 # Core ML logic\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 data.py                   # Dataset loading, transforms\n\u2502   \u2502   \u251c\u2500\u2500 loss.py                   # Custom loss functions\n\u2502   \u2502   \u251c\u2500\u2500 main.py                   # Entrypoint script\n\u2502   \u2502   \u251c\u2500\u2500 model.py                  # Model architectures\n\u2502   \u2502   \u2514\u2500\u2500 train.py                  # Training loop\n\u2502   \u2514\u2500\u2500 tests/                        # Tests\n\u2502       \u251c\u2500\u2500 test_data1.py\n\u2502       \u251c\u2500\u2500 test_model.py\n\u2502       \u251c\u2500\u2500 test_training.py\n\u2502       \u251c\u2500\u2500 tests_integration/        # Integration-level tests\n\u2502       \u2502   \u251c\u2500\u2500 api_testing.py\n\u2502       \u2502   \u2514\u2500\u2500 locustfile.py         # Load testing with Locust\n\u2502       \u2514\u2500\u2500 tests_unit/               # Unit-level tests\n\u2502           \u251c\u2500\u2500 test_data.py\n\u2502           \u2514\u2500\u2500 test_train.py\n\u2514\u2500\u2500 .github/                          # GitHub CI/CD config\n    \u251c\u2500\u2500 dependabot.yaml               # Dependency update config\n    \u2514\u2500\u2500 workflows/                    # GitHub Actions workflows\n        \u251c\u2500\u2500 ci.yml                    # Main CI pipeline\n        \u251c\u2500\u2500 data-changes.yaml         # DVC-based data CI triggers\n        \u2514\u2500\u2500 model-deploy.yml          # DVC-based model W&amp;B CI triggers\n</code></pre>"},{"location":"source/data/","title":"Data Module","text":"<p>This module handles the data loading from the datasets and data version control and preprocessing for food segmentation.</p>"},{"location":"source/data/#dvc-setup-and-configuration","title":"DVC Setup and Configuration","text":""},{"location":"source/data/#data-storage-architecture","title":"Data Storage Architecture","text":"<p>The project uses DVC (Data Version Control) with Google Cloud Storage for data versioning and management:</p> <ul> <li>Data storage: <code>gs://dvc-storage-sensor/</code></li> <li>Model storage: <code>gs://food-segmentation-models/</code></li> </ul>"},{"location":"source/data/#initial-dvc-setup","title":"Initial DVC Setup","text":"<pre><code># Install required tools\npip install dvc-gs\n\n# List available GCS buckets\ngsutil ls\n\n# Add remote storage\ndvc remote add -d remote_storage gs://dvc-storage-sensor/\n\n# Configure version-aware storage\ndvc remote modify remote_storage version_aware true\n\n# Pull data from remote storage\ndvc pull\n</code></pre>"},{"location":"source/data/#common-dvc-issues-and-solutions","title":"Common DVC Issues and Solutions","text":""},{"location":"source/data/#known-problems-encountered","title":"Known Problems Encountered","text":"<ol> <li> <p>Push Command Failures <code>bash    # This command often fails    dvc push --no-cache</code></p> </li> <li> <p>Remote Configuration Issues    ```bash    # Remove problematic remotes    dvc remote remove gcp_storage</p> </li> </ol> <p># Set correct default remote    dvc remote default remote_storage    ```</p> <ol> <li>Authentication Problems</li> <li>Ensure Google Cloud SDK is properly configured</li> <li>Verify bucket permissions and access rights</li> </ol>"},{"location":"source/data/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># List configured remotes\ndvc remote list\n\n# Check remote configuration\ndvc remote list --show-origin\n\n# Verify data status\ndvc status\n\n# Force refresh from remote\ndvc fetch --all-commits\n</code></pre>"},{"location":"source/data/#alternative-data-access","title":"Alternative Data Access","text":"<p>If DVC setup encounters persistent issues, you can:</p> <ol> <li> <p>Direct GCS Access <code>bash    # Access data directly from GCS buckets    gsutil -m cp -r gs://dvc-storage-sensor/data/ ./data/</code></p> </li> <li> <p>Manual Dataset Download</p> </li> <li>Download Food103 segmentation dataset from: https://paperswithcode.com/dataset/foodseg103</li> <li>Extract and place in <code>data/</code> directory</li> </ol>"},{"location":"source/data/#data-module-integration","title":"Data Module Integration","text":"<pre><code>:::src.segmentation.data\n</code></pre> <p>The data module integrates with DVC by: - Automatically checking for DVC-tracked files - Handling both local and remote data sources - Providing fallback mechanisms when DVC is unavailable - Managing data preprocessing pipelines with version control</p>"},{"location":"source/data/#best-practices","title":"Best Practices","text":"<ul> <li>Always verify DVC remote configuration before starting work</li> <li>Use <code>dvc status</code> to check data synchronization</li> <li>Keep <code>.dvc</code> files in version control</li> <li>Test data access in development environment before production deployment</li> </ul> <p>This setup ensures reproducible data management while providing flexibility when DVC encounters configuration issues.</p>"},{"location":"source/loss/","title":"Loss Functions","text":"<p>Loss functions and evaluation metrics for segmentation.</p>"},{"location":"source/loss/#src.segmentation.loss","title":"<code>src.segmentation.loss</code>","text":""},{"location":"source/loss/#src.segmentation.loss.iou","title":"<code>iou(pred, target)</code>","text":"<p>Calculate Intersection over Union (IoU) for semantic segmentation.</p> <p>Memory efficient version optimized for very large images/batches. Only processes classes that are present in the current batch to save computation.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted segmentation masks. Can be either: - 4D tensor (B, C, H, W) with class probabilities/logits - 3D tensor (B, H, W) with class indices</p> required <code>target</code> <code>Tensor</code> <p>Ground truth segmentation masks with class indices. Shape: (B, H, W) where each value is a class index.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: IoU scores for each class (shape: [104,]). Classes not present in the batch will have NaN values. Only classes present in pred or target will have computed IoU scores.</p> Note <ul> <li>Assumes 104 total classes (food segmentation classes)</li> <li>Automatically handles conversion from logits to class predictions</li> <li>Memory efficient by only computing IoU for classes present in batch</li> <li>Returns results on CPU as numpy array for compatibility</li> </ul> Example <p>pred = torch.rand(2, 104, 224, 224)  # Batch of 2, 104 classes target = torch.randint(0, 104, (2, 224, 224))  # Ground truth iou_scores = iou(pred, target) print(f\"Mean IoU: {np.nanmean(iou_scores):.3f}\")</p> Source code in <code>src/segmentation/loss.py</code> <pre><code>def iou(pred, target):\n    \"\"\"\n    Calculate Intersection over Union (IoU) for semantic segmentation.\n\n    Memory efficient version optimized for very large images/batches.\n    Only processes classes that are present in the current batch to save computation.\n\n    Args:\n        pred (torch.Tensor): Predicted segmentation masks. Can be either:\n            - 4D tensor (B, C, H, W) with class probabilities/logits\n            - 3D tensor (B, H, W) with class indices\n        target (torch.Tensor): Ground truth segmentation masks with class indices.\n            Shape: (B, H, W) where each value is a class index.\n\n    Returns:\n        numpy.ndarray: IoU scores for each class (shape: [104,]).\n            Classes not present in the batch will have NaN values.\n            Only classes present in pred or target will have computed IoU scores.\n\n    Note:\n        - Assumes 104 total classes (food segmentation classes)\n        - Automatically handles conversion from logits to class predictions\n        - Memory efficient by only computing IoU for classes present in batch\n        - Returns results on CPU as numpy array for compatibility\n\n    Example:\n        &gt;&gt;&gt; pred = torch.rand(2, 104, 224, 224)  # Batch of 2, 104 classes\n        &gt;&gt;&gt; target = torch.randint(0, 104, (2, 224, 224))  # Ground truth\n        &gt;&gt;&gt; iou_scores = iou(pred, target)\n        &gt;&gt;&gt; print(f\"Mean IoU: {np.nanmean(iou_scores):.3f}\")\n    \"\"\"\n    num_classes = 104\n\n    if pred.dim() == 4:\n        pred = torch.argmax(pred, dim=1)  # max class value along each pixel\n\n    pred_flat = pred.view(-1)\n    target_flat = target.view(-1)\n\n    # Find unique classes present in this batch\n    unique_classes = torch.unique(torch.cat([pred_flat, target_flat]))\n    unique_classes = unique_classes[unique_classes &lt; num_classes]\n\n    # Initialize full IoU array with NaN\n    iou_results = torch.full((num_classes,), float(\"nan\"), device=pred.device)\n\n    # Only calculate IoU for classes present in batch\n    for cls in unique_classes:\n        pred_cls = pred_flat == cls\n        target_cls = target_flat == cls\n\n        intersection = (pred_cls &amp; target_cls).sum().float()\n        union = (pred_cls | target_cls).sum().float()\n\n        if union &gt; 0:\n            iou_results[cls] = intersection / union\n\n    return iou_results.cpu().numpy()\n</code></pre>"},{"location":"source/main/","title":"Main Pipeline","text":"<p>Main training pipeline with Hydra configuration management for food segmentation.</p>"},{"location":"source/main/#cli-and-hydra-configuration-management","title":"CLI and Hydra Configuration Management","text":"<p>This module demonstrates advanced CLI configuration management using Hydra and OmegaConf, providing flexible parameter override capabilities and structured configuration handling.</p>"},{"location":"source/main/#key-configuration-features","title":"Key Configuration Features","text":"<p>Hydra Integration:</p> <pre><code>@hydra.main(version_base=None, config_path=config_dir, config_name=\"config\")\ndef main(cfg: DictConfig):\n</code></pre> <p>Dynamic Configuration Loading: - Automatically resolves config directory from project root - Loads configuration from <code>configs/config.yaml</code> - Supports hierarchical configuration structures</p>"},{"location":"source/main/#cli-usage-examples","title":"CLI Usage Examples","text":"<p>Default Configuration:</p> <pre><code>python src/segmentation/main.py\n</code></pre> <p>Parameter Override:</p> <pre><code>python src/segmentation/main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001\n</code></pre> <p>Multiple Parameters:</p> <pre><code>python src/segmentation/main.py model.hyperparameters.epochs=100 model.hyperparameters.batch_size=32 paths.base_dir=/custom/path\n</code></pre>"},{"location":"source/main/#configuration-structure","title":"Configuration Structure","text":"<p>The pipeline expects configuration with the following structure:</p> <pre><code>model:\n  hyperparameters:\n    epochs: 20\n    batch_size: 16\n    lr: 0.0001\n\npaths:\n  base_dir: \"data/\"\n\nprofiling:\n  enabled: false\n</code></pre>"},{"location":"source/main/#enhanced-user-experience","title":"Enhanced User Experience","text":"<p>Rich Console Integration: - Displays formatted hyperparameter tables - Provides colorized progress indicators - Shows training status with visual panels</p> <p>Automatic Environment Setup: - Configures Weights &amp; Biases (wandb) in silent mode - Sets up proper Python path resolution - Handles project root discovery</p>"},{"location":"source/main/#pipeline-workflow","title":"Pipeline Workflow","text":"<ol> <li>Configuration Loading - Hydra loads and validates config</li> <li>Parameter Display - Rich table shows current hyperparameters  </li> <li>Data Loading - Initializes train/test data loaders</li> <li>Training Execution - Runs complete training pipeline</li> <li>Visualization Generation - Creates metrics and prediction plots</li> <li>Completion - Displays success confirmation</li> </ol> <p>This approach provides reproducible experiments with easy parameter tuning through CLI overrides, making it ideal for hyperparameter sweeps and experiment tracking.</p> <pre><code>:::src.segmentation.main\n</code></pre> <p>Sources</p>"},{"location":"source/model/","title":"Model Architecture","text":"<p>Implementation of the MiniUNet segmentation model.</p>"},{"location":"source/model/#src.segmentation.model","title":"<code>src.segmentation.model</code>","text":""},{"location":"source/model/#src.segmentation.model.MiniUNet","title":"<code>MiniUNet()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Lightweight U-Net architecture for semantic segmentation.</p> <p>A simplified version of the U-Net architecture designed for food segmentation with 104 output classes. Features encoder-decoder structure with skip connections and proper weight initialization.</p> Architecture <ul> <li>Encoder: 3 conv blocks with max pooling (3\u219264\u2192128\u2192256 channels)</li> <li>Bottleneck: 1 conv block (256\u2192512 channels)</li> <li>Decoder: 3 conv blocks with transpose convolutions (512\u2192256\u2192128\u219264 channels)</li> <li>Output: 1x1 conv to 104 classes</li> </ul> <p>Attributes:</p> Name Type Description <code>encoder1,</code> <code>(encoder2, encoder3)</code> <p>Encoder convolutional blocks</p> <code>bottleneck</code> <p>Bottleneck convolutional block</p> <code>decoder1,</code> <code>(decoder2, decoder3)</code> <p>Decoder convolutional blocks</p> <code>pool</code> <p>Max pooling layer for downsampling</p> <code>upconv1,</code> <code>(upconv2, upconv3)</code> <p>Transpose convolutions for upsampling</p> <code>final</code> <p>Final 1x1 convolution to output classes</p> Example <p>model = MiniUNet() x = torch.rand(1, 3, 224, 224)  # Batch of 1, RGB image output = model(x) print(output.shape)  # torch.Size([1, 104, 224, 224])</p> <p>Initialize the MiniUNet model.</p> <p>Sets up the encoder-decoder architecture with skip connections, pooling/upsampling layers, and applies He weight initialization.</p> Source code in <code>src/segmentation/model.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the MiniUNet model.\n\n    Sets up the encoder-decoder architecture with skip connections,\n    pooling/upsampling layers, and applies He weight initialization.\n    \"\"\"\n    super(MiniUNet, self).__init__()\n\n    # Encoder\n    self.encoder1 = self.conv_block(3, 64)\n    self.encoder2 = self.conv_block(64, 128)\n    self.encoder3 = self.conv_block(128, 256)\n\n    # Bottleneck\n    self.bottleneck = self.conv_block(256, 512)\n\n    # Decoder\n    self.decoder1 = self.conv_block(512, 256)\n    self.decoder2 = self.conv_block(256, 128)\n    self.decoder3 = self.conv_block(128, 64)\n\n    # Pooling and upsampling\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    # Upsampling\n    self.upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n    self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n    self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n\n    # Final layer\n    self.final = nn.Sequential(\n        nn.Conv2d(64, 104, kernel_size=1),\n    )\n\n    # \u2705 Initialize weights\n    self._initialize_weights()\n</code></pre>"},{"location":"source/model/#src.segmentation.model.MiniUNet.conv_block","title":"<code>conv_block(in_channels, out_channels)</code>","text":"<p>Create a convolutional block with two conv layers and ReLU activations.</p> <p>Each block consists of two 3x3 convolutions with padding, followed by ReLU activations. This design increases the receptive field while maintaining spatial dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <p>Returns:</p> Type Description <p>nn.Sequential: Sequential container with conv layers and ReLU activations</p> Note <p>Using two conv layers increases the receptive field and adds non-linearity without increasing parameters significantly.</p> Source code in <code>src/segmentation/model.py</code> <pre><code>def conv_block(self, in_channels, out_channels):\n    \"\"\"\n    Create a convolutional block with two conv layers and ReLU activations.\n\n    Each block consists of two 3x3 convolutions with padding, followed by\n    ReLU activations. This design increases the receptive field while\n    maintaining spatial dimensions.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n\n    Returns:\n        nn.Sequential: Sequential container with conv layers and ReLU activations\n\n    Note:\n        Using two conv layers increases the receptive field and adds\n        non-linearity without increasing parameters significantly.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n    )\n</code></pre>"},{"location":"source/model/#src.segmentation.model.MiniUNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the MiniUNet model.</p> <p>Implements the U-Net architecture with encoder-decoder structure and skip connections. The encoder progressively reduces spatial dimensions while increasing channel depth. The decoder upsamples and combines features using skip connections.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor with shape (B, 3, H, W) where: - B: batch size - 3: RGB channels - H, W: height and width</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Segmentation logits with shape (B, 104, H, W) where: - B: batch size - 104: number of food classes - H, W: same as input dimensions</p> Architecture Flow <ol> <li>Encoder: x \u2192 enc1 \u2192 enc2 \u2192 enc3</li> <li>Bottleneck: enc3 \u2192 bottleneck</li> <li>Decoder: bottleneck + enc3 \u2192 dec1 + enc2 \u2192 dec2 + enc1 \u2192 dec3</li> <li>Output: dec3 \u2192 final (104 classes)</li> </ol> Source code in <code>src/segmentation/model.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass through the MiniUNet model.\n\n    Implements the U-Net architecture with encoder-decoder structure\n    and skip connections. The encoder progressively reduces spatial\n    dimensions while increasing channel depth. The decoder upsamples\n    and combines features using skip connections.\n\n    Args:\n        x (torch.Tensor): Input tensor with shape (B, 3, H, W) where:\n            - B: batch size\n            - 3: RGB channels\n            - H, W: height and width\n\n    Returns:\n        torch.Tensor: Segmentation logits with shape (B, 104, H, W) where:\n            - B: batch size\n            - 104: number of food classes\n            - H, W: same as input dimensions\n\n    Architecture Flow:\n        1. Encoder: x \u2192 enc1 \u2192 enc2 \u2192 enc3\n        2. Bottleneck: enc3 \u2192 bottleneck\n        3. Decoder: bottleneck + enc3 \u2192 dec1 + enc2 \u2192 dec2 + enc1 \u2192 dec3\n        4. Output: dec3 \u2192 final (104 classes)\n    \"\"\"\n    # Encoder\n    enc1 = self.encoder1(x)\n    enc2 = self.encoder2(self.pool(enc1))\n    enc3 = self.encoder3(self.pool(enc2))\n\n    # Bottleneck\n    bottleneck = self.bottleneck(self.pool(enc3))\n\n    # Decoder with skip connections\n    dec1 = self.decoder1(torch.cat([self.upconv1(bottleneck), enc3], dim=1))\n    dec2 = self.decoder2(torch.cat([self.upconv2(dec1), enc2], dim=1))\n    dec3 = self.decoder3(torch.cat([self.upconv3(dec2), enc1], dim=1))\n\n    return self.final(dec3)\n</code></pre>"},{"location":"source/training/","title":"Training Module","text":"<p>Comprehensive training pipeline with metrics tracking and visualization for food segmentation.</p>"},{"location":"source/training/#what-we-are-tracking","title":"What We Are Tracking","text":""},{"location":"source/training/#core-metrics","title":"Core Metrics","text":"<ul> <li>Loss Functions - Training and validation loss per epoch</li> <li>Segmentation Accuracy - Pixel-wise accuracy and mean IoU</li> <li>Learning Progress - Learning rate schedules and training time</li> <li>Model Performance - Validation metrics and best model checkpointing</li> </ul>"},{"location":"source/training/#experiment-tracking","title":"Experiment Tracking","text":"<ul> <li>Weights &amp; Biases Integration - Hyperparameters, model architecture, and system metrics</li> <li>Visualization Outputs - Training curves, loss plots, and prediction visualizations</li> </ul>"},{"location":"source/training/#tracking-architecture","title":"Tracking Architecture","text":"<p>Hybrid approach combining: - Local logging with Rich console output - Weights &amp; Biases for cloud-based experiment tracking - File-based visualization saves - Model checkpoint management</p> <p>This focuses on essential segmentation metrics while maintaining training pipeline simplicity.</p> <pre><code>:::src.segmentation.train\n</code></pre>"}]}