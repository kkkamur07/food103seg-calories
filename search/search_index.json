{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Food103Seg Calories","text":"<p>Welcome to Food103Seg Calories - a deep learning project for food image segmentation and calorie estimation.</p>"},{"location":"#live-demo","title":"\ud83d\ude80 Live Demo","text":"<p>Try the application now! Our Streamlit app is live at: the link</p>"},{"location":"#overview","title":"Overview","text":"<p>This project uses computer vision to identify different food items in images and estimate their caloric content. Built with PyTorch and featuring a MiniUNet architecture, it can segment 104 different food categories with high accuracy.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udf55 Food Segmentation: Identifies and segments different food items in images</li> <li>\ud83d\udcca Calorie Estimation: Calculates caloric content from segmented food regions</li> <li>\ud83e\udde0 MiniUNet Model: Lightweight U-Net architecture optimized for food segmentation</li> <li>\ud83c\udf10 Web Interface: User-friendly Streamlit application</li> <li>\u2699\ufe0f Easy Configuration: Hydra-based configuration management</li> <li>\ud83d\udcc8 Experiment Tracking: Integration with Weights &amp; Biases</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install dependencies\npip install -r requirements.txt\n\n# Train the model\npython src/segmentation/main.py\n\n# Launch web app\nstreamlit run src/app/frontend.py\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 segmentation/          # Core ML modules\n\u2502   \u2502   \u251c\u2500\u2500 data.py           # Data loading\n\u2502   \u2502   \u251c\u2500\u2500 model.py          # MiniUNet architecture\n\u2502   \u2502   \u251c\u2500\u2500 train.py          # Training pipeline\n\u2502   \u2502   \u2514\u2500\u2500 main.py           # Main training script\n\u2502   \u2514\u2500\u2500 app/                  # Web application\n\u2502       \u251c\u2500\u2500 frontend.py       # Streamlit interface\n\u2502       \u2514\u2500\u2500 service.py        # API service\n\u251c\u2500\u2500 configs/                  # Configuration files\n\u251c\u2500\u2500 notebooks/                # Jupyter notebooks\n\u2514\u2500\u2500 docs/                     # Documentation\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Installation - Set up the project</li> <li>Quick Start - Get running quickly</li> <li>API Reference - Explore the code</li> <li>Training Guide - Train your own models</li> </ol>"},{"location":"#model-performance","title":"Model Performance","text":"<ul> <li>104 Food Classes: Comprehensive food category coverage</li> <li>High Accuracy: &gt;85% mean IoU on test set</li> <li>Fast Inference: ~50ms per image on GPU</li> <li>Lightweight: ~15MB model size</li> </ul> <p>Ready to start? Check out our installation guide or try the live demo!</p> <p>Sources</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#live-demo","title":"\ud83d\ude80 Live Demo","text":"<p>Try the application now! Our Streamlit app is live at: the link</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing the Food103Seg Calories project, ensure you have the following prerequisites:</p> <ul> <li>Python 3.8+ (Python 3.9 or 3.10 recommended)</li> <li>CUDA-compatible GPU (recommended for training, optional for inference)</li> <li>Git for cloning the repository</li> <li>pip package manager</li> </ul>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"Component Minimum Recommended Python 3.8+ 3.9 or 3.10 RAM 8GB 16GB+ GPU Memory 4GB 8GB+ Storage 10GB 20GB+"},{"location":"installation/#installation-steps","title":"Installation Steps","text":""},{"location":"installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone &lt;your-repo-url&gt;\ncd kkkamur07-food103seg-calories\n</code></pre>"},{"location":"installation/#2-create-virtual-environment-recommended","title":"2. Create Virtual Environment (Recommended)","text":"<p>Using venv:</p> <pre><code>python -m venv food-seg-env\nsource food-seg-env/bin/activate  # On Windows: food-seg-env\\Scripts\\activate\n</code></pre> <p>Using conda:</p> <pre><code>conda create -n food-seg-env python=3.9\nconda activate food-seg-env\n</code></pre>"},{"location":"installation/#3-install-dependencies","title":"3. Install Dependencies","text":""},{"location":"installation/#production-installation","title":"Production Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<pre><code>pip install -r requirements_dev.txt\n</code></pre>"},{"location":"installation/#install-project-in-development-mode","title":"Install Project in Development Mode","text":"<pre><code>pip install -e .\n</code></pre>"},{"location":"installation/#4-gpu-setup-optional-but-recommended","title":"4. GPU Setup (Optional but Recommended)","text":"<p>For CUDA support, install PyTorch with CUDA:</p> <pre><code># For CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# For CUDA 12.1\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>Verify GPU installation:</p> <pre><code>python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"installation/#data-setup","title":"Data Setup","text":""},{"location":"installation/#1-download-dataset","title":"1. Download Dataset","text":"<p>Download the Food103 segmentation dataset and place it in the <code>data/</code> directory:</p> <pre><code>mkdir -p data\n# Download your dataset here\n</code></pre>"},{"location":"installation/#2-expected-directory-structure","title":"2. Expected Directory Structure","text":"<p>Ensure your data follows this structure:</p> <pre><code>data/\n\u251c\u2500\u2500 Images/\n\u2502   \u251c\u2500\u2500 img_dir/\n\u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 test/\n\u2502   \u2502       \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502       \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ann_dir/\n\u2502       \u251c\u2500\u2500 train/\n\u2502       \u2502   \u251c\u2500\u2500 image1.png\n\u2502       \u2502   \u251c\u2500\u2500 image2.png\n\u2502       \u2502   \u2514\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 test/\n\u2502           \u251c\u2500\u2500 image1.png\n\u2502           \u251c\u2500\u2500 image2.png\n\u2502           \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"installation/#configuration-setup","title":"Configuration Setup","text":""},{"location":"installation/#1-create-required-directories","title":"1. Create Required Directories","text":"<pre><code>mkdir -p saved/{logs,models,reports,predictions}\nmkdir -p configs/outputs\n</code></pre>"},{"location":"installation/#2-environment-variables-optional","title":"2. Environment Variables (Optional)","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># .env\nWANDB_PROJECT=Food-Segmentation\nCUDA_VISIBLE_DEVICES=0\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":""},{"location":"installation/#1-test-installation","title":"1. Test Installation","text":"<p>Run the following commands to verify your installation:</p> <pre><code># Test imports\npython -c \"import torch; import torchvision; print('PyTorch installed successfully')\"\n\n# Test project modules\npython -c \"from src.segmentation.data import data_loaders; print('Project modules working')\"\n\n# Test data loading\npython -c \"from src.segmentation.data import data_loaders; print('Data loading test passed')\"\n</code></pre>"},{"location":"installation/#2-quick-training-test","title":"2. Quick Training Test","text":"<p>Run a quick training test with minimal epochs:</p> <pre><code>python src/segmentation/main.py model.hyperparameters.epochs=1\n</code></pre>"},{"location":"installation/#running-the-application","title":"Running the Application","text":""},{"location":"installation/#1-streamlit-web-app","title":"1. Streamlit Web App","text":"<pre><code>streamlit run src/app/frontend.py\n</code></pre>"},{"location":"installation/#2-training-pipeline","title":"2. Training Pipeline","text":"<pre><code>python src/segmentation/main.py\n</code></pre>"},{"location":"installation/#3-custom-training","title":"3. Custom Training","text":"<pre><code>python src/segmentation/main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":""},{"location":"installation/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<ul> <li>Reduce batch size in config: <code>model.hyperparameters.batch_size=16</code></li> <li>Use CPU training: <code>CUDA_VISIBLE_DEVICES=\"\" python src/segmentation/main.py</code></li> </ul>"},{"location":"installation/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code>pip install --upgrade pip\npip install -r requirements.txt --force-reinstall\n</code></pre>"},{"location":"installation/#data-loading-errors","title":"Data Loading Errors","text":"<ul> <li>Verify directory structure matches expected format</li> <li>Check file permissions: <code>chmod -R 755 data/</code></li> <li>Ensure image and annotation files have correct extensions</li> </ul>"},{"location":"installation/#import-errors","title":"Import Errors","text":"<pre><code># Reinstall in development mode\npip install -e . --force-reinstall\n</code></pre>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the logs in <code>saved/logs/</code></li> <li>Verify GPU setup with <code>nvidia-smi</code></li> <li>Review configuration in <code>configs/config.yaml</code></li> <li>Check Python version compatibility</li> </ol>"},{"location":"installation/#optional-components","title":"Optional Components","text":""},{"location":"installation/#docker-setup","title":"Docker Setup","text":"<p>If you prefer Docker:</p> <pre><code># Build backend\ndocker build -f Dockerfile.backend -t food-seg-backend .\n\n# Build frontend\ndocker build -f Dockerfile.frontend -t food-seg-frontend .\n\n# Run with docker-compose\ndocker-compose up\n</code></pre>"},{"location":"installation/#development-tools","title":"Development Tools","text":"<p>Install additional development tools:</p> <pre><code># Pre-commit hooks\npre-commit install\n\n# Jupyter for notebooks\npip install jupyter\njupyter notebook notebooks/\n</code></pre>"},{"location":"installation/#performance-optimization","title":"Performance Optimization","text":""},{"location":"installation/#for-training","title":"For Training","text":"<ul> <li>Use mixed precision: Add <code>--mixed-precision</code> flag</li> <li>Increase batch size if you have enough GPU memory</li> <li>Use multiple GPUs with <code>CUDA_VISIBLE_DEVICES=0,1</code></li> </ul>"},{"location":"installation/#for-inference","title":"For Inference","text":"<ul> <li>Use TorchScript for faster inference</li> <li>Batch processing for multiple images</li> <li>CPU inference for deployment scenarios</li> </ul>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Review the configuration in <code>configs/config.yaml</code></li> <li>Run the training pipeline with your data</li> <li>Explore the Streamlit app at the live link above</li> <li>Check the documentation for advanced usage</li> <li>Set up monitoring with Weights &amp; Biases</li> </ol> <p>You're now ready to start training your food segmentation model and estimating calories! \ud83c\udf55\ud83d\udcca</p> <p>Sources</p>"},{"location":"structure/","title":"Project Structure","text":"<pre><code>kkkamur07-food103seg-calories/\n\u251c\u2500\u2500 README.md                           # Project overview and instructions\n\u251c\u2500\u2500 cloudbuild.yaml                    # Google Cloud Build config\n\u251c\u2500\u2500 data.dvc                           # DVC-tracked data file\n\u251c\u2500\u2500 docker-compose.yml                 # Orchestration of backend + frontend\n\u251c\u2500\u2500 Dockerfile.backend                 # Dockerfile for backend service\n\u251c\u2500\u2500 Dockerfile.frontend                # Dockerfile for frontend app\n\u251c\u2500\u2500 project_structure.txt              # Describes the project structure\n\u251c\u2500\u2500 pyproject.toml                     # Python project metadata + build system\n\u251c\u2500\u2500 requirements.txt                   # Production dependencies\n\u251c\u2500\u2500 requirements_dev.txt              # Dev dependencies (linting, testing)\n\u251c\u2500\u2500 tasks.py                           # Automation scripts (e.g. via `invoke`)\n\u251c\u2500\u2500 uv.lock                            # Dependency lock file for uv tool\n\u251c\u2500\u2500 wandb_runner.py                    # W&amp;B experiment runner\n\u251c\u2500\u2500 .dockerignore                      # Ignore rules for Docker builds\n\u251c\u2500\u2500 .dvcignore                         # Ignore rules for DVC\n\u251c\u2500\u2500 .pre-commit-config.yaml           # Pre-commit hooks config\n\u251c\u2500\u2500 configs/                           # All project configs\n\u2502   \u251c\u2500\u2500 config.yaml                    # Main config file (training, paths)\n\u2502   \u251c\u2500\u2500 wandb_sweep.yaml              # W&amp;B sweep configuration\n\u2502   \u251c\u2500\u2500 dataset/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml              # Dataset-specific config\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u2502   \u2514\u2500\u2500 default.yaml              # Model-specific config\n\u2502   \u2514\u2500\u2500 outputs/                      # Experiment outputs\n\u2502       \u251c\u2500\u2500 2025-07-02/\n\u2502       \u2502   \u2514\u2500\u2500 22-43-00/\n\u2502       \u2502       \u251c\u2500\u2500 wandb/           # W&amp;B run logs\n\u2502       \u2502       \u2514\u2500\u2500 .hydra/          # Hydra config snapshots\n\u2502       \u2502           \u251c\u2500\u2500 config.yaml\n\u2502       \u2502           \u2514\u2500\u2500 hydra.yaml\n\u2502       \u2514\u2500\u2500 2025-07-04/\n\u2502           \u2514\u2500\u2500 21-10-21/\n\u2502               \u2514\u2500\u2500 .hydra/\n\u2502                   \u2514\u2500\u2500 hydra.yaml\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 experiment.ipynb              # Jupyter notebook for experiments\n\u251c\u2500\u2500 saved/\n\u2502   \u2514\u2500\u2500 models.dvc                    # Tracked model file(s) with DVC\n\u251c\u2500\u2500 src/                              # Source code\n\u2502   \u251c\u2500\u2500 app/                          # Application code (serving)\n\u2502   \u2502   \u251c\u2500\u2500 bentoml.py               # BentoML service definition\n\u2502   \u2502   \u251c\u2500\u2500 bentoml_setup.py        # BentoML setup utility\n\u2502   \u2502   \u251c\u2500\u2500 frontend.py             # Streamlit or Gradio frontend\n\u2502   \u2502   \u251c\u2500\u2500 frontend_requirements.txt\n\u2502   \u2502   \u2514\u2500\u2500 service.py              # Service logic\n\u2502   \u251c\u2500\u2500 segmentation/                # Core ML logic\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 data.py                 # Dataset loading, transforms\n\u2502   \u2502   \u251c\u2500\u2500 loss.py                 # Custom loss functions\n\u2502   \u2502   \u251c\u2500\u2500 main.py                 # Entrypoint script\n\u2502   \u2502   \u251c\u2500\u2500 model.py                # Model architectures\n\u2502   \u2502   \u2514\u2500\u2500 train.py                # Training loop\n\u2502   \u2514\u2500\u2500 tests/                       # Tests\n\u2502       \u251c\u2500\u2500 test_data1.py\n\u2502       \u251c\u2500\u2500 test_model.py\n\u2502       \u251c\u2500\u2500 test_training.py\n\u2502       \u251c\u2500\u2500 tests_integration/      # Integration-level tests\n\u2502       \u2502   \u251c\u2500\u2500 api_testing.py\n\u2502       \u2502   \u2514\u2500\u2500 locustfile.py       # Load testing with Locust\n\u2502       \u2514\u2500\u2500 tests_unit/             # Unit-level tests\n\u2502           \u251c\u2500\u2500 test_data.py\n\u2502           \u2514\u2500\u2500 test_train.py\n\u2514\u2500\u2500 .github/                          # GitHub CI/CD config\n    \u251c\u2500\u2500 dependabot.yaml              # Dependency update config\n    \u2514\u2500\u2500 workflows/                   # GitHub Actions workflows\n        \u251c\u2500\u2500 ci.yml                   # Main CI pipeline\n        \u251c\u2500\u2500 data-changes.yaml       # DVC-based data CI triggers\n        \u2514\u2500\u2500 model-deploy.yml        # Model deployment pipeline\n</code></pre>"},{"location":"source/data/","title":"Data Module","text":"<p>This module handles the data loading from the datasets and dataversion control and preprocessing for food segmentation. </p>"},{"location":"source/data/#src.segmentation.data","title":"<code>src.segmentation.data</code>","text":""},{"location":"source/data/#src.segmentation.data.FoodSegDataset","title":"<code>FoodSegDataset(base_dir, mode='train', transforms=None, ann_transform=None)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch Dataset for food image segmentation.</p> <p>Loads images and corresponding annotation (mask) files for segmentation tasks.</p> <p>Attributes:</p> Name Type Description <code>base_dir</code> <code>str</code> <p>Root project directory.</p> <code>mode</code> <code>str</code> <p>\"train\" or \"test\". Used to pick subfolders.</p> <code>transforms</code> <code>callable</code> <p>Image transformation pipeline.</p> <code>ann_transform</code> <code>callable</code> <p>Annotation/mask transformation pipeline.</p> <code>images</code> <code>list</code> <p>List of image filenames.</p> <code>annotations</code> <code>list</code> <p>List of annotation filenames.</p> <p>Initialize the FoodSegDataset.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Project's root directory.</p> required <code>mode</code> <code>str</code> <p>Either \"train\" or \"test\".</p> <code>'train'</code> <code>transforms</code> <code>callable</code> <p>Transformations for input images.</p> <code>None</code> <code>ann_transform</code> <code>callable</code> <p>Transformations for annotation masks.</p> <code>None</code> Source code in <code>src/segmentation/data.py</code> <pre><code>def __init__(self, base_dir, mode=\"train\", transforms=None, ann_transform=None):\n    \"\"\"\n    Initialize the FoodSegDataset.\n\n    Args:\n        base_dir (str): Project's root directory.\n        mode (str): Either \"train\" or \"test\".\n        transforms (callable, optional): Transformations for input images.\n        ann_transform (callable, optional): Transformations for annotation masks.\n    \"\"\"\n    self.base_dir = base_dir\n    self.data_dir = os.path.join(base_dir, \"data\")\n    self.mode = mode  # 'train' or 'test'\n\n    self.img_dir = os.path.join(self.data_dir, \"Images\", \"img_dir\", mode)\n    self.ann_dir = os.path.join(self.data_dir, \"Images\", \"ann_dir\", mode)\n\n    self.transforms = transforms\n    self.ann_transform = ann_transform\n    self.images = os.listdir(self.img_dir)\n    self.annotations = os.listdir(self.ann_dir)\n</code></pre>"},{"location":"source/data/#src.segmentation.data.FoodSegDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Fetch image and annotation mask by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the item to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(image, mask) - Transformed image and segmentation mask tensors.</p> Source code in <code>src/segmentation/data.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Fetch image and annotation mask by index.\n\n    Args:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        tuple: (image, mask) - Transformed image and segmentation mask tensors.\n    \"\"\"\n    img_name = os.path.join(self.img_dir, self.images[idx])\n    ann_name = os.path.join(self.ann_dir, self.images[idx].replace(\".jpg\", \".png\"))\n\n    # Load image\n    image = Image.open(img_name).convert(\"RGB\")\n    if self.transforms:\n        image = self.transforms(image)\n\n    # Load mask\n    mask = Image.open(ann_name).convert(\"L\")\n    if self.ann_transform:\n        mask = self.ann_transform(mask)\n\n    return image, mask\n</code></pre>"},{"location":"source/data/#src.segmentation.data.FoodSegDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of images (samples).</p> Source code in <code>src/segmentation/data.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Return the number of samples in the dataset.\n\n    Returns:\n        int: Number of images (samples).\n    \"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"source/data/#src.segmentation.data.data_loaders","title":"<code>data_loaders(base_dir, batch_size, num_workers=4)</code>","text":"<p>Prepare PyTorch DataLoaders for training and testing.</p> <p>Creates datasets with appropriate transformations and returns DataLoaders for both training and testing phases of the segmentation model.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Root project directory.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders.</p> required <code>num_workers</code> <code>int</code> <p>Number of subprocesses to use for data loading.</p> <code>4</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(train_loader, test_loader) - DataLoaders for training and testing.</p> Source code in <code>src/segmentation/data.py</code> <pre><code>def data_loaders(base_dir, batch_size, num_workers=4):\n    \"\"\"\n    Prepare PyTorch DataLoaders for training and testing.\n\n    Creates datasets with appropriate transformations and returns DataLoaders\n    for both training and testing phases of the segmentation model.\n\n    Args:\n        base_dir (str): Root project directory.\n        batch_size (int): Batch size for DataLoaders.\n        num_workers (int): Number of subprocesses to use for data loading.\n\n    Returns:\n        tuple: (train_loader, test_loader) - DataLoaders for training and testing.\n    \"\"\"\n\n    ann_transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.PILToTensor(),\n            transforms.Lambda(lambda x: x.squeeze().long()),\n            transforms.Lambda(\n                lambda x: torch.where(x &gt;= 104, torch.tensor(0, dtype=torch.long), x)\n            ),\n        ]\n    )\n\n    img_transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),  # converts PIL to float tensor and scales to [0, 1]\n        ]\n    )\n\n    # Create the dataset\n    train_dataset = FoodSegDataset(\n        base_dir, mode=\"train\", transforms=img_transform, ann_transform=ann_transform\n    )\n    test_dataset = FoodSegDataset(\n        base_dir, mode=\"test\", transforms=img_transform, ann_transform=ann_transform\n    )\n\n    # DataLoaders\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n    )\n\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n    )\n\n    print(f\"Train dataset size: {len(train_dataset)}\")\n    print(f\"Test dataset size: {len(test_dataset)}\")\n\n    return train_loader, test_loader\n</code></pre>"},{"location":"source/loss/","title":"Loss Functions","text":"<p>Loss functions and evaluation metrics for segmentation.</p>"},{"location":"source/loss/#src.segmentation.loss","title":"<code>src.segmentation.loss</code>","text":""},{"location":"source/loss/#src.segmentation.loss.iou","title":"<code>iou(pred, target)</code>","text":"<p>Calculate Intersection over Union (IoU) for semantic segmentation.</p> <p>Memory efficient version optimized for very large images/batches. Only processes classes that are present in the current batch to save computation.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted segmentation masks. Can be either: - 4D tensor (B, C, H, W) with class probabilities/logits - 3D tensor (B, H, W) with class indices</p> required <code>target</code> <code>Tensor</code> <p>Ground truth segmentation masks with class indices. Shape: (B, H, W) where each value is a class index.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: IoU scores for each class (shape: [104,]). Classes not present in the batch will have NaN values. Only classes present in pred or target will have computed IoU scores.</p> Note <ul> <li>Assumes 104 total classes (food segmentation classes)</li> <li>Automatically handles conversion from logits to class predictions</li> <li>Memory efficient by only computing IoU for classes present in batch</li> <li>Returns results on CPU as numpy array for compatibility</li> </ul> Example <p>pred = torch.rand(2, 104, 224, 224)  # Batch of 2, 104 classes target = torch.randint(0, 104, (2, 224, 224))  # Ground truth iou_scores = iou(pred, target) print(f\"Mean IoU: {np.nanmean(iou_scores):.3f}\")</p> Source code in <code>src/segmentation/loss.py</code> <pre><code>def iou(pred, target):\n    \"\"\"\n    Calculate Intersection over Union (IoU) for semantic segmentation.\n\n    Memory efficient version optimized for very large images/batches.\n    Only processes classes that are present in the current batch to save computation.\n\n    Args:\n        pred (torch.Tensor): Predicted segmentation masks. Can be either:\n            - 4D tensor (B, C, H, W) with class probabilities/logits\n            - 3D tensor (B, H, W) with class indices\n        target (torch.Tensor): Ground truth segmentation masks with class indices.\n            Shape: (B, H, W) where each value is a class index.\n\n    Returns:\n        numpy.ndarray: IoU scores for each class (shape: [104,]).\n            Classes not present in the batch will have NaN values.\n            Only classes present in pred or target will have computed IoU scores.\n\n    Note:\n        - Assumes 104 total classes (food segmentation classes)\n        - Automatically handles conversion from logits to class predictions\n        - Memory efficient by only computing IoU for classes present in batch\n        - Returns results on CPU as numpy array for compatibility\n\n    Example:\n        &gt;&gt;&gt; pred = torch.rand(2, 104, 224, 224)  # Batch of 2, 104 classes\n        &gt;&gt;&gt; target = torch.randint(0, 104, (2, 224, 224))  # Ground truth\n        &gt;&gt;&gt; iou_scores = iou(pred, target)\n        &gt;&gt;&gt; print(f\"Mean IoU: {np.nanmean(iou_scores):.3f}\")\n    \"\"\"\n    num_classes = 104\n\n    if pred.dim() == 4:\n        pred = torch.argmax(pred, dim=1)  # max class value along each pixel\n\n    pred_flat = pred.view(-1)\n    target_flat = target.view(-1)\n\n    # Find unique classes present in this batch\n    unique_classes = torch.unique(torch.cat([pred_flat, target_flat]))\n    unique_classes = unique_classes[unique_classes &lt; num_classes]\n\n    # Initialize full IoU array with NaN\n    iou_results = torch.full((num_classes,), float(\"nan\"), device=pred.device)\n\n    # Only calculate IoU for classes present in batch\n    for cls in unique_classes:\n        pred_cls = pred_flat == cls\n        target_cls = target_flat == cls\n\n        intersection = (pred_cls &amp; target_cls).sum().float()\n        union = (pred_cls | target_cls).sum().float()\n\n        if union &gt; 0:\n            iou_results[cls] = intersection / union\n\n    return iou_results.cpu().numpy()\n</code></pre>"},{"location":"source/main/","title":"Main Pipeline","text":"<p>Main training pipeline with Hydra configuration management.</p>"},{"location":"source/main/#src.segmentation.main","title":"<code>src.segmentation.main</code>","text":""},{"location":"source/main/#src.segmentation.main.main","title":"<code>main(cfg)</code>","text":"<p>Main training pipeline for food segmentation model.</p> <p>Orchestrates the complete training workflow including data loading, model training, and visualization generation. Uses Hydra for configuration management and Rich for enhanced console output.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object containing all training parameters, paths, and settings. Expected structure: - cfg.model.hyperparameters: Training hyperparameters - cfg.paths.base_dir: Base directory for data and outputs - cfg.profiling.enabled: Whether to enable performance profiling</p> required Pipeline Steps <ol> <li>Load and display configuration parameters</li> <li>Initialize data loaders for training and testing</li> <li>Create and configure trainer instance</li> <li>Execute training loop</li> <li>Generate training metrics visualizations</li> <li>Generate prediction visualizations</li> </ol> Example <p>Run with default config:</p> <p>python main.py</p> <p>Run with custom parameters:</p> <p>python main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001</p> Source code in <code>src/segmentation/main.py</code> <pre><code>@hydra.main(version_base=None, config_path=config_dir, config_name=\"config\")\ndef main(cfg: DictConfig):\n    \"\"\"\n    Main training pipeline for food segmentation model.\n\n    Orchestrates the complete training workflow including data loading,\n    model training, and visualization generation. Uses Hydra for configuration\n    management and Rich for enhanced console output.\n\n    Args:\n        cfg (DictConfig): Hydra configuration object containing all training\n            parameters, paths, and settings. Expected structure:\n            - cfg.model.hyperparameters: Training hyperparameters\n            - cfg.paths.base_dir: Base directory for data and outputs\n            - cfg.profiling.enabled: Whether to enable performance profiling\n\n    Pipeline Steps:\n        1. Load and display configuration parameters\n        2. Initialize data loaders for training and testing\n        3. Create and configure trainer instance\n        4. Execute training loop\n        5. Generate training metrics visualizations\n        6. Generate prediction visualizations\n\n    Example:\n        Run with default config:\n        &gt;&gt;&gt; python main.py\n\n        Run with custom parameters:\n        &gt;&gt;&gt; python main.py model.hyperparameters.epochs=50 model.hyperparameters.lr=0.001\n    \"\"\"\n    epochs = cfg.model.hyperparameters.epochs\n    batch_size = cfg.model.hyperparameters.batch_size\n    learning_rate = cfg.model.hyperparameters.lr\n    base_dir = cfg.paths.base_dir\n    profiling_enabled = cfg.profiling.enabled\n    print_hyperparameters_table(cfg)\n\n    console.print(\n        Panel.fit(\"\ud83c\udf55 Food Segmentation Training Pipeline\", style=\"bold green\")\n    )\n    console.print()\n\n    console.print(\"[yellow]\ud83d\udcca Loading data...\")\n    train_loader, test_loader = data_loaders(\n        base_dir=base_dir, batch_size=batch_size, num_workers=4\n    )\n    console.print(\n        f\"[green]\u2713 Train: {len(train_loader.dataset)} samples | Test: {len(test_loader.dataset)} samples[/green]\"\n    )\n    console.print()\n\n    console.print(\"[blue]\ud83d\ude80 Starting training...[/blue]\")\n    trainer = Trainer(\n        epochs=epochs,\n        base_dir=base_dir,\n        batch_size=batch_size,\n        lr=learning_rate,\n        enable_profiler=profiling_enabled,\n        init_wandb=True,\n    )\n\n    trainer.train()\n\n    console.print(\"[magenta]\ud83d\udcc8 Generating visualizations...[/magenta]\")\n    trainer.visualize_training_metrics()\n    trainer.visualize_predictions()\n\n    console.print(\n        Panel.fit(\"\u2705 Training Pipeline Completed Successfully!\", style=\"bold green\")\n    )\n</code></pre>"},{"location":"source/main/#src.segmentation.main.print_hyperparameters_table","title":"<code>print_hyperparameters_table(cfg)</code>","text":"<p>Display hyperparameters in a clean table format using Rich.</p> <p>Creates a formatted table showing key training configuration parameters including epochs, batch size, learning rate, and base directory path.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object containing model hyperparameters and path settings. Expected to have structure: - cfg.model.hyperparameters.epochs - cfg.model.hyperparameters.batch_size - cfg.model.hyperparameters.lr - cfg.paths.base_dir</p> required Example <p>print_hyperparameters_table(cfg)</p> Source code in <code>src/segmentation/main.py</code> <pre><code>def print_hyperparameters_table(cfg: DictConfig):\n    \"\"\"\n    Display hyperparameters in a clean table format using Rich.\n\n    Creates a formatted table showing key training configuration parameters\n    including epochs, batch size, learning rate, and base directory path.\n\n    Args:\n        cfg (DictConfig): Hydra configuration object containing model hyperparameters\n            and path settings. Expected to have structure:\n            - cfg.model.hyperparameters.epochs\n            - cfg.model.hyperparameters.batch_size\n            - cfg.model.hyperparameters.lr\n            - cfg.paths.base_dir\n\n    Example:\n        &gt;&gt;&gt; print_hyperparameters_table(cfg)\n        # Displays a formatted table with training configuration\n    \"\"\"\n    table = Table(\n        title=\"\ud83d\udd27 Training Configuration\", show_header=True, header_style=\"bold blue\"\n    )\n    table.add_column(\"Parameter\", style=\"cyan\", width=20)\n    table.add_column(\"Value\", style=\"white\", width=15)\n\n    table.add_row(\"Epochs\", str(cfg.model.hyperparameters.epochs))\n    table.add_row(\"Batch Size\", str(cfg.model.hyperparameters.batch_size))\n    table.add_row(\"Learning Rate\", str(cfg.model.hyperparameters.lr))\n    table.add_row(\"Base Directory\", str(cfg.paths.base_dir))\n\n    console.print(table)\n    console.print()\n</code></pre>"},{"location":"source/main/#src.segmentation.main.print_hyperparameters_table--displays-a-formatted-table-with-training-configuration","title":"Displays a formatted table with training configuration","text":""},{"location":"source/model/","title":"Model Architecture","text":"<p>Implementation of the MiniUNet segmentation model.</p>"},{"location":"source/model/#src.segmentation.model","title":"<code>src.segmentation.model</code>","text":""},{"location":"source/model/#src.segmentation.model.MiniUNet","title":"<code>MiniUNet()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Lightweight U-Net architecture for semantic segmentation.</p> <p>A simplified version of the U-Net architecture designed for food segmentation with 104 output classes. Features encoder-decoder structure with skip connections and proper weight initialization.</p> Architecture <ul> <li>Encoder: 3 conv blocks with max pooling (3\u219264\u2192128\u2192256 channels)</li> <li>Bottleneck: 1 conv block (256\u2192512 channels)</li> <li>Decoder: 3 conv blocks with transpose convolutions (512\u2192256\u2192128\u219264 channels)</li> <li>Output: 1x1 conv to 104 classes</li> </ul> <p>Attributes:</p> Name Type Description <code>encoder1,</code> <code>(encoder2, encoder3)</code> <p>Encoder convolutional blocks</p> <code>bottleneck</code> <p>Bottleneck convolutional block</p> <code>decoder1,</code> <code>(decoder2, decoder3)</code> <p>Decoder convolutional blocks</p> <code>pool</code> <p>Max pooling layer for downsampling</p> <code>upconv1,</code> <code>(upconv2, upconv3)</code> <p>Transpose convolutions for upsampling</p> <code>final</code> <p>Final 1x1 convolution to output classes</p> Example <p>model = MiniUNet() x = torch.rand(1, 3, 224, 224)  # Batch of 1, RGB image output = model(x) print(output.shape)  # torch.Size([1, 104, 224, 224])</p> <p>Initialize the MiniUNet model.</p> <p>Sets up the encoder-decoder architecture with skip connections, pooling/upsampling layers, and applies He weight initialization.</p> Source code in <code>src/segmentation/model.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the MiniUNet model.\n\n    Sets up the encoder-decoder architecture with skip connections,\n    pooling/upsampling layers, and applies He weight initialization.\n    \"\"\"\n    super(MiniUNet, self).__init__()\n\n    # Encoder\n    self.encoder1 = self.conv_block(3, 64)\n    self.encoder2 = self.conv_block(64, 128)\n    self.encoder3 = self.conv_block(128, 256)\n\n    # Bottleneck\n    self.bottleneck = self.conv_block(256, 512)\n\n    # Decoder\n    self.decoder1 = self.conv_block(512, 256)\n    self.decoder2 = self.conv_block(256, 128)\n    self.decoder3 = self.conv_block(128, 64)\n\n    # Pooling and upsampling\n    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    # Upsampling\n    self.upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n    self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n    self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n\n    # Final layer\n    self.final = nn.Sequential(\n        nn.Conv2d(64, 104, kernel_size=1),\n    )\n\n    # \u2705 Initialize weights\n    self._initialize_weights()\n</code></pre>"},{"location":"source/model/#src.segmentation.model.MiniUNet.conv_block","title":"<code>conv_block(in_channels, out_channels)</code>","text":"<p>Create a convolutional block with two conv layers and ReLU activations.</p> <p>Each block consists of two 3x3 convolutions with padding, followed by ReLU activations. This design increases the receptive field while maintaining spatial dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <p>Returns:</p> Type Description <p>nn.Sequential: Sequential container with conv layers and ReLU activations</p> Note <p>Using two conv layers increases the receptive field and adds non-linearity without increasing parameters significantly.</p> Source code in <code>src/segmentation/model.py</code> <pre><code>def conv_block(self, in_channels, out_channels):\n    \"\"\"\n    Create a convolutional block with two conv layers and ReLU activations.\n\n    Each block consists of two 3x3 convolutions with padding, followed by\n    ReLU activations. This design increases the receptive field while\n    maintaining spatial dimensions.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n\n    Returns:\n        nn.Sequential: Sequential container with conv layers and ReLU activations\n\n    Note:\n        Using two conv layers increases the receptive field and adds\n        non-linearity without increasing parameters significantly.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n    )\n</code></pre>"},{"location":"source/model/#src.segmentation.model.MiniUNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the MiniUNet model.</p> <p>Implements the U-Net architecture with encoder-decoder structure and skip connections. The encoder progressively reduces spatial dimensions while increasing channel depth. The decoder upsamples and combines features using skip connections.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor with shape (B, 3, H, W) where: - B: batch size - 3: RGB channels - H, W: height and width</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Segmentation logits with shape (B, 104, H, W) where: - B: batch size - 104: number of food classes - H, W: same as input dimensions</p> Architecture Flow <ol> <li>Encoder: x \u2192 enc1 \u2192 enc2 \u2192 enc3</li> <li>Bottleneck: enc3 \u2192 bottleneck</li> <li>Decoder: bottleneck + enc3 \u2192 dec1 + enc2 \u2192 dec2 + enc1 \u2192 dec3</li> <li>Output: dec3 \u2192 final (104 classes)</li> </ol> Source code in <code>src/segmentation/model.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass through the MiniUNet model.\n\n    Implements the U-Net architecture with encoder-decoder structure\n    and skip connections. The encoder progressively reduces spatial\n    dimensions while increasing channel depth. The decoder upsamples\n    and combines features using skip connections.\n\n    Args:\n        x (torch.Tensor): Input tensor with shape (B, 3, H, W) where:\n            - B: batch size\n            - 3: RGB channels\n            - H, W: height and width\n\n    Returns:\n        torch.Tensor: Segmentation logits with shape (B, 104, H, W) where:\n            - B: batch size\n            - 104: number of food classes\n            - H, W: same as input dimensions\n\n    Architecture Flow:\n        1. Encoder: x \u2192 enc1 \u2192 enc2 \u2192 enc3\n        2. Bottleneck: enc3 \u2192 bottleneck\n        3. Decoder: bottleneck + enc3 \u2192 dec1 + enc2 \u2192 dec2 + enc1 \u2192 dec3\n        4. Output: dec3 \u2192 final (104 classes)\n    \"\"\"\n    # Encoder\n    enc1 = self.encoder1(x)\n    enc2 = self.encoder2(self.pool(enc1))\n    enc3 = self.encoder3(self.pool(enc2))\n\n    # Bottleneck\n    bottleneck = self.bottleneck(self.pool(enc3))\n\n    # Decoder with skip connections\n    dec1 = self.decoder1(torch.cat([self.upconv1(bottleneck), enc3], dim=1))\n    dec2 = self.decoder2(torch.cat([self.upconv2(dec1), enc2], dim=1))\n    dec3 = self.decoder3(torch.cat([self.upconv3(dec2), enc1], dim=1))\n\n    return self.final(dec3)\n</code></pre>"},{"location":"source/training/","title":"Training Module","text":"<p>Comprehensive training pipeline with metrics tracking and visualization.</p>"},{"location":"source/training/#src.segmentation.train","title":"<code>src.segmentation.train</code>","text":""},{"location":"source/training/#src.segmentation.train.Trainer","title":"<code>Trainer(lr=None, epochs=None, batch_size=None, base_dir=None, enable_profiler=None, init_wandb=True)</code>","text":"<p>Initialize the Trainer and do experimental logging with Weights and Biases.</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>None</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>None</code> <code>base_dir</code> <code>str</code> <p>Project's root directory.</p> <code>None</code> <code>enable_profiler</code> <code>bool</code> <p>Whether to enable PyTorch profiler.</p> <code>None</code> <code>init_wandb</code> <code>bool</code> <p>Whether to initialize Weights and Biases.</p> <code>True</code> Source code in <code>src/segmentation/train.py</code> <pre><code>def __init__(\n    self,\n    lr=None,\n    epochs=None,\n    batch_size=None,\n    base_dir=None,\n    enable_profiler=None,\n    init_wandb=True,\n):\n    \"\"\"\n    Initialize the Trainer and do experimental logging with Weights and Biases.\n\n    Args:\n        lr (float): Learning rate for the optimizer.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        base_dir (str): Project's root directory.\n        enable_profiler (bool): Whether to enable PyTorch profiler.\n        init_wandb (bool): Whether to initialize Weights and Biases.\n    \"\"\"\n\n    super().__init__()\n    # model\n    self.model = MiniUNet().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # Model Parameters\n    self.parameters = sum(\n        p.numel() for p in self.model.parameters() if p.requires_grad\n    )\n\n    # Training Parameters\n    self.epochs = epochs\n    self.lr = lr\n    self.batch_size = batch_size\n\n    # Directories\n    self.base_dir = base_dir\n    self.saved_dir = os.path.join(self.base_dir, \"saved\")\n    self.model_path = os.path.join(self.saved_dir, \"models\", \"model.pth\")\n    self.plots_path = os.path.join(\n        self.saved_dir, \"reports\", \"training_metrics.png\"\n    )\n    self.predictions = os.path.join(\n        self.saved_dir, \"predictions\", \"predictions.png\"\n    )\n    # Data Loaders\n    self.train_loader, self.test_loader = data_loaders(\n        base_dir=self.base_dir,\n        batch_size=self.batch_size,\n    )\n\n    # Loss function\n    self.loss = nn.CrossEntropyLoss()\n    self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n\n    # Training History\n    self.train_losses = []\n    self.test_losses = []\n    self.train_accs = []\n    self.test_accs = []\n    self.train_ious = []\n    self.test_ious = []\n    self.best_test_loss = float(\"inf\")\n\n    logger.info(f\"Model initialized with {self.parameters} trainable parameters.\")\n\n    self.init_wandb = init_wandb\n    self.enable_profiler = enable_profiler\n    if not self.enable_profiler:\n        logger.info(\"Profiler is disabled. Training will run without profiling.\")\n\n    # Initialize Weights and Biases\n    if self.init_wandb and wandb.run is None:\n        wandb.init(\n            project=\"Food-Segmentation\",\n            config={\n                \"epochs\": self.epochs,\n                \"learning_rate\": self.lr,\n                \"batch_size\": self.batch_size,\n                \"base_dir\": self.base_dir,\n                \"trainable_params\": self.parameters,\n                \"model\": \"MiniUNet\",\n                \"optimizer\": \"Adam\",\n                \"loss_function\": \"CrossEntropyLoss\",\n            },\n        )\n        wandb.watch(self.model, log=\"all\")\n        logger.info(\"Weights and Biases initialized for tracking.\")\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.calculate_iou","title":"<code>calculate_iou(pred_mask, true_mask, num_classes=104)</code>","text":"<p>Implements the Intersection over Union (IoU) metric for segmentation tasks.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def calculate_iou(self, pred_mask, true_mask, num_classes=104):\n    \"\"\"Implements the Intersection over Union (IoU) metric for segmentation tasks.\"\"\"\n    ious = []\n    pred_mask = pred_mask.view(-1)\n    true_mask = true_mask.view(-1)\n\n    for cls in range(num_classes):\n        pred_inds = pred_mask == cls\n        target_inds = true_mask == cls\n\n        intersection = (pred_inds &amp; target_inds).long().sum().item()\n        union = (pred_inds | target_inds).long().sum().item()\n\n        if union == 0:\n            continue  # Skip if no pixels for this class\n\n        iou = intersection / union\n        ious.append(iou)\n\n    return np.mean(ious) if ious else 0.0\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the model.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def forward(self, x):\n    \"\"\" Forward pass through the model.\"\"\"\n    return self.model(x)\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.train","title":"<code>train()</code>","text":"<p>Execute the training loop with optional profiling and Weights and Biases logging.</p> <p>Pipeline Steps :  1. Initialize the device for training (GPU or CPU). 2. Check if the model path is set. 3. Create profiler if enabled. 4. Loop through the number of epochs: 5. Train the model on the training dataset. 6. Calculate and log training metrics (loss, accuracy, IoU). 7. Validate the model on the test dataset. 8. Save the best model based on test loss. 9. Log metrics to Weights and Biases. 10. Finish Weights and Biases run.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def train(self):\n    \"\"\"\n\n    Execute the training loop with optional profiling and Weights and Biases logging.\n\n    Pipeline Steps : \n    1. Initialize the device for training (GPU or CPU).\n    2. Check if the model path is set.\n    3. Create profiler if enabled.\n    4. Loop through the number of epochs:\n    5. Train the model on the training dataset.\n    6. Calculate and log training metrics (loss, accuracy, IoU).\n    7. Validate the model on the test dataset.\n    8. Save the best model based on test loss.\n    9. Log metrics to Weights and Biases.\n    10. Finish Weights and Biases run.\n\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if not self.model_path:\n        logger.warning(\"Model path is not set. Cannot save the model.\")\n        return\n\n    os.makedirs(\"./profiler_logs\", exist_ok=True)\n\n    prof = None\n\n    if self.enable_profiler:\n        prof = profile(\n            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n            schedule=schedule(wait=1, warmup=1, active=3, repeat=2),\n            on_trace_ready=torch.profiler.tensorboard_trace_handler(\n                \"./profiler_logs\"\n            ),\n            record_shapes=True,\n            profile_memory=True,\n            with_stack=True,\n        )\n        prof.start()\n\n    for epoch in range(self.epochs):\n        # Training\n        self.model.train()\n\n        running_loss = 0.0\n        running_accuracy = 0.0\n        running_iou = 0.0\n\n        train_bar = tqdm(\n            self.train_loader,\n            desc=f\"Epoch {epoch+1}/{self.epochs} [Train]\",\n            leave=False,\n        )\n\n        for batch_idx, (images, masks) in enumerate(train_bar):\n            images = images.to(device)\n            masks = masks.to(device).long()\n\n            self.optimizer.zero_grad()\n            outputs = self.forward(images)\n            loss = self.loss(outputs, masks)\n            loss.backward()\n            self.optimizer.step()\n\n            # Calculate accuracy for current batch\n            pred_classes = torch.argmax(outputs, dim=1)\n            train_accuracy = torch.mean((pred_classes == masks).float())\n            train_iou = self.calculate_iou(pred_classes, masks)\n\n            # Accumulate loss and accuracy\n            running_accuracy += train_accuracy.item() * images.size(0)\n            running_loss += loss.item() * images.size(0)\n            running_iou += train_iou * images.size(0)\n\n            # Update progress bar\n            train_bar.set_postfix(loss=loss.item())\n\n            # Step profiler and limit batches for profiling\n            if self.enable_profiler and prof is not None:\n                prof.step()\n                logger.info(f\"Profiler step {batch_idx + 1}\")\n                if batch_idx &gt;= 20:\n                    logger.info(\"\ud83d\udd25 Profiling completed\")\n                    prof.stop()\n                    break\n\n        # Training metrics\n        train_loss = running_loss / len(self.train_loader.dataset)\n        train_acc = running_accuracy / len(self.train_loader.dataset)\n        train_iou = running_iou / len(self.train_loader.dataset)\n        self.train_losses.append(train_loss)\n        self.train_accs.append(train_acc)\n        self.train_ious.append(train_iou)\n\n        # Your existing validation code...\n        self.model.eval()\n        running_loss = 0.0\n        running_accuracy = 0.0\n        running_iou = 0.0\n\n        test_bar = tqdm(\n            self.test_loader,\n            desc=f\"Epoch {epoch+1}/{self.epochs} [Test]\",\n            leave=False,\n        )\n\n        with torch.no_grad():\n            for images, masks in test_bar:\n                images = images.to(device)\n                masks = masks.to(device).long()\n\n                outputs = self.forward(images)\n                loss = self.loss(outputs, masks)\n\n                pred_classes = torch.argmax(outputs, dim=1)\n                test_accuracy = torch.mean((pred_classes == masks).float())\n                test_iou = self.calculate_iou(pred_classes, masks)\n\n                running_accuracy += test_accuracy.item() * images.size(0)\n                running_loss += loss.item() * images.size(0)\n                running_iou += test_iou * images.size(0)\n\n                test_bar.set_postfix(test_loss=loss.item())\n\n        test_loss = running_loss / len(self.test_loader.dataset)\n        test_acc = running_accuracy / len(self.test_loader.dataset)\n        test_iou = running_iou / len(self.test_loader.dataset)\n\n        self.test_losses.append(test_loss)\n        self.test_accs.append(test_acc)\n        self.test_ious.append(test_iou)\n\n        print(f\"Epoch [{epoch+1}/{self.epochs}]:\")\n        print(\n            f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train IoU: {train_iou:.4f}\"\n        )\n        print(\n            f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test IoU: {test_iou:.4f}\"\n        )\n\n        wandb.log(\n            {\n                \"Train Loss\": train_loss,\n                \"Train Accuracy\": train_acc,\n                \"Test Loss\": test_loss,\n                \"Test Accuracy\": test_acc,\n                \"epoch\": epoch + 1,\n                \"Train IoU\": train_iou,\n                \"Test IoU\": test_iou,\n            }\n        )\n\n        if test_loss &lt; self.best_test_loss:\n            self.best_test_loss = test_loss\n            torch.save(\n                {\n                    \"model_state_dict\": self.model.state_dict(),\n                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n                    \"epoch\": epoch + 1,\n                    \"best_test_loss\": self.best_test_loss,\n                    \"final_train_loss\": train_loss,\n                    \"final_test_loss\": test_loss,\n                    \"final_train_acc\": train_acc,\n                    \"final_test_acc\": test_acc,\n                },\n                self.model_path,\n            )\n            logger.info(f\"Best model saved with test_loss: {test_loss:.4f}\")\n\n            artifact = wandb.Artifact(\n                \"model\",\n                type=\"model\",\n                description=\"Best model based on test loss\",\n            )\n            artifact.add_file(self.model_path)\n            wandb.log_artifact(artifact)\n\n        logger.info(f\"Training complete. Model saved at {self.model_path}\")\n        print(\"Training complete.\")\n\n    wandb.finish()\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.visualize_predictions","title":"<code>visualize_predictions(num_images=5)</code>","text":"<p>This function generates a grid of input images, ground truth masks, and model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>num_images</code> <code>int</code> <p>Number of images to visualize.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Saves the prediction grid to the specified path.</p> Source code in <code>src/segmentation/train.py</code> <pre><code>def visualize_predictions(self, num_images=5):\n    \"\"\"\n    This function generates a grid of input images, ground truth masks, and model predictions.\n\n    Args:\n        num_images (int): Number of images to visualize.\n\n    Returns:\n        None: Saves the prediction grid to the specified path.\n\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.model.eval()\n\n    if not self.predictions:\n        logger.warning(\"No predictions path provided. Predictions not saved.\")\n        return\n\n    images_to_plot = []\n    masks_to_plot = []\n    preds_to_plot = []\n\n    with torch.no_grad():\n        for i, (images, masks) in enumerate(self.test_loader):\n            if i &gt;= num_images:\n                break\n\n            images = images.to(device)\n            masks = masks.to(device).long()\n\n            outputs = self.forward(images)\n            pred_classes = torch.argmax(outputs, dim=1)\n\n            # Store first image from each batch\n            image = images[0].cpu()\n\n            if image.max() &gt; 1.0:\n                image = image / 255.0\n\n            images_to_plot.append(image.permute(1, 2, 0).numpy())\n            masks_to_plot.append(masks[0].cpu().numpy())\n            preds_to_plot.append(pred_classes[0].cpu().numpy())\n\n    _, axes = plt.subplots(3, num_images, figsize=(5 * num_images, 15))\n\n    for i in range(num_images):\n        # Input images\n        axes[0, i].imshow(np.clip(images_to_plot[i], 0, 1))\n        axes[0, i].set_title(f\"Input Image {i+1}\")\n        axes[0, i].axis(\"off\")\n\n        # Ground truth masks\n        axes[1, i].imshow(masks_to_plot[i], cmap=\"viridis\", vmin=0, vmax=103)\n        axes[1, i].set_title(f\"Ground Truth {i+1}\")\n        axes[1, i].axis(\"off\")\n\n        # Predictions\n        axes[2, i].imshow(preds_to_plot[i], cmap=\"viridis\", vmin=0, vmax=103)\n        axes[2, i].set_title(f\"Prediction {i+1}\")\n        axes[2, i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.savefig(self.predictions, dpi=300, bbox_inches=\"tight\")\n    logger.info(f\"Prediction grid saved: {self.predictions}\")\n    plt.show()\n    plt.close()\n</code></pre>"},{"location":"source/training/#src.segmentation.train.Trainer.visualize_training_metrics","title":"<code>visualize_training_metrics()</code>","text":"<p>Visualize training and testing loss &amp; accuracy from saved model checkpoint Creates two side-by-side graphs: Loss comparison and Accuracy comparison</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <p>Base directory for saving plots</p> required <code>model_path</code> <p>Path to the saved model checkpoint (.pth file)</p> required <code>plots_path</code> <p>Directory to save the plots</p> required Source code in <code>src/segmentation/train.py</code> <pre><code>def visualize_training_metrics(self):\n    \"\"\"\n    Visualize training and testing loss &amp; accuracy from saved model checkpoint\n    Creates two side-by-side graphs: Loss comparison and Accuracy comparison\n\n    Args:\n        base_dir: Base directory for saving plots\n        model_path: Path to the saved model checkpoint (.pth file)\n        plots_path: Directory to save the plots\n    \"\"\"\n\n    # Extract metrics\n    train_losses = self.train_losses\n    test_losses = self.test_losses\n    train_accs = self.train_accs\n    test_accs = self.test_accs\n    train_ious = self.train_ious\n    test_ious = self.test_ious\n\n    # Check if data exists\n    if not train_losses or not test_losses:\n        logger.warning(\"No loss data found in checkpoint!\")\n        return\n\n    if not train_accs or not test_accs:\n        logger.warning(\"No accuracy data found in checkpoint!\")\n        return\n\n    if not self.plots_path:\n        logger.warning(\"No plots path provided. Plot not saved.\")\n        return\n\n    # Create figure with 3 subplots side by side\n    _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n\n    epochs = range(1, len(train_losses) + 1)\n\n    # Graph 1: Training vs Testing Loss\n    ax1.plot(\n        epochs,\n        train_losses,\n        label=\"Training Loss\",\n        color=\"blue\",\n        marker=\"o\",\n        linewidth=2,\n    )\n    ax1.plot(\n        epochs,\n        test_losses,\n        label=\"Testing Loss\",\n        color=\"red\",\n        marker=\"s\",\n        linewidth=2,\n    )\n    ax1.set_title(\"Loss Comparison\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend()\n    ax1.grid(True)\n\n    # Graph 2: Training vs Testing Accuracy\n    ax2.plot(\n        epochs,\n        train_accs,\n        label=\"Training Accuracy\",\n        color=\"green\",\n        marker=\"o\",\n        linewidth=2,\n    )\n    ax2.plot(\n        epochs,\n        test_accs,\n        label=\"Testing Accuracy\",\n        color=\"orange\",\n        marker=\"s\",\n        linewidth=2,\n    )\n    ax2.set_title(\"Accuracy Comparison\")\n    ax2.set_xlabel(\"Epochs\")\n    ax2.set_ylabel(\"Accuracy\")\n    ax2.legend()\n    ax2.grid(True)\n\n    # Graph 3: Training vs Testing IoU\n    ax3.plot(\n        epochs,\n        train_ious,\n        label=\"Training IoU\",\n        color=\"purple\",\n        marker=\"o\",\n        linewidth=2,\n    )\n    ax3.plot(\n        epochs,\n        test_ious,\n        label=\"Testing IoU\",\n        color=\"brown\",\n        marker=\"s\",\n        linewidth=2,\n    )\n    ax3.set_title(\"IoU Comparison\")\n    ax3.set_xlabel(\"Epochs\")\n    ax3.set_ylabel(\"IoU\")\n    ax3.legend()\n    ax3.grid(True)\n\n    # Adjust layout and save the plot\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.85)  # Adjust top to make space for the title\n    plt.suptitle(\"Training and Testing Metrics\", fontsize=20)\n    plt.savefig(self.plots_path, dpi=300)  # Save the plot with high resolution\n    logger.info(f\"Training metrics plot saved: {self.plots_path}\")\n    plt.tight_layout()\n    plt.show()  # Show the plot in interactive mode\n    plt.close()  # Close the plot to free memory\n</code></pre>"}]}